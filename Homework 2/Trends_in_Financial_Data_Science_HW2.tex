\documentclass[11pt]{article}

\usepackage{amsmath,amsthm,amssymb}

%%%%% Matrix stretcher
% use it as:
%\begin{pmatrix}[1.5]
% ...
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}


%%%%%%%%%%%%% Colors %%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}

\definecolor{C0}{HTML}{1d1d1d}
\definecolor{C1}{HTML}{1e3668}
\definecolor{C2}{HTML}{199d8b}
\definecolor{C3}{HTML}{d52f4c}
\definecolor{C4}{HTML}{5ab2d6}
\definecolor{C5}{HTML}{ffb268}
\definecolor{C6}{HTML}{ff7300} % for commenting - {fire orange}dd571c
\definecolor{C7}{HTML}{777b7e} % for remarks - {steel grey}
\color{C0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%% Fonts %%%%%%%%%%%%% 
%\usepackage{fontspec}
\usepackage[no-math]{fontspec} % for text

\emergencystretch=8pt
\hyphenpenalty=1000 % default 50
\tolerance=800      % default 200
%\righthyphenmin=4
%\lefthyphenmin=4

%%% Text Font: Vollkorn + Math Font: Latin Modern (default) %%%
\setmainfont{Vollkorn}[
UprightFont = Vollkorn-Regular,
ItalicFont =Vollkorn-Italic, 
BoldItalicFont={Vollkorn-BoldItalic},
BoldFont = Vollkorn-Bold,
RawFeature=+lnum,
WordSpace=1.7,
] 

%%% We need this for math font packages other than latin modern %%%
% \usepackage{unicode-math}        % for math

%%% Text Font: Palatino + Math Font: Asana-Math %%%
%\setmainfont{Palatino}[
%BoldFont = Palatino-Bold,
%ItalicFont = Palatino-Italic,
%BoldItalicFont={Palatino-BoldItalic},
%RawFeature=+lnum,
%WordSpace=1.7,
%]
%\setmathfont{asana-math}

%%% Text Font: Arno Pro + Math Font: Minion Pro %%%
%\setmainfont{Arno Pro}[
%UprightFont = *-Regular,
%ItalicFont = Vollkorn-Italic, 
%BoldItalicFont={*-BoldItalic},
%BoldFont = *-Bold,
%RawFeature=+lnum,
%WordSpace=1.7,
%Scale= 1.1
%] 
% Minion Pro is too expensive

%%% Math Fonts %%%
%\setmathfont{Vollkorn}
%\setmathfont{Latin Modern Math}
%\setmathfont{TeX Gyre Pagella Math}
%\setmathfont{TeX Gyre Termes Math}
%\setmathfont{TeX Gyre DejaVu Math}
%\setmathfont[Scale=MatchLowercase]{DejaVu Math TeX Gyre}
%\setmathfont{XITS Math}
%\setmathfont{Libertinus Math}
%\setmathfont[Scale=MatchUppercase]{Asana Math}
%\setmathfont{STIX Two Math}

%\usepackage{kpfonts-otf}
%\setmathfont{KpMath-Regular.otf}[version=regular]
%\setmathfont{KpMath-Bold.otf}[version=bold]
%\setmathfont{KpMath-Semibold.otf}[version=semibold]
%\setmathfont{KpMath-Sans.otf}[version=sans]
%\setmathfont{KpMath-Light.otf}[version=light]


%%% CJK Fonts %%%
\usepackage[scale=.78]{luatexja-fontspec}
\setmainjfont{BabelStone Han}[AutoFakeBold]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package simplifies the insertion of external multi-page PDF or PS documents.
\usepackage{pdfpages}

% cref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=C4,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\usepackage[nameinlink,noabbrev,capitalize]{cleveref}
% \crefname{ineq}{}{}
% \crefname{equation}{}{}
% \creflabelformat{ineq}{#2{\textup{(1)}}#3}
% \creflabelformat{equation}{#2\textup{(#1)}#3}

%%%%%%%%%%%%% Environments %%%%%%%%%%%%%%%%
%amsthm has three separate predefined styles:	
%
%\theoremstyle{plain} is the default. it sets the text in italic and adds extra space above and below the \newtheorems listed below it in the input. it is recommended for theorems, corollaries, lemmas, propositions, conjectures, criteria, and (possibly; depends on the subject area) algorithms.
%
%\theoremstyle{definition} adds extra space above and below, but sets the text in roman. it is recommended for definitions, conditions, problems, and examples; i've alse seen it used for exercises.
%
%\theoremstyle{remark} is set in roman, with no additional space above or below. it is recommended for remarks, notes, notation, claims, summaries, acknowledgments, cases, and conclusions.

%%%  theorem-like environment %%%
\theoremstyle{plain} % default theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\newtheorem{definition}[theorem]{Definition}

%%% definition-like environment %%%
%\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}


%%% framed package is great %%%
\usepackage{framed}
\newenvironment{solution}
{\color{C2}\normalfont\begin{framed}\begingroup\textbf{Solution:} }
  {\endgroup\end{framed}}

\newenvironment{topic}
  {\color{C2}\normalfont\begin{framed}\begingroup}
    {\endgroup\end{framed}}


\newtheoremstyle{remark}% name of the style to be used
  {}% measure of space to leave above the theorem. E.g.: 3pt
  {}% measure of space to leave below the theorem. E.g.: 3pt
  {\color{C3}}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\color{C3}\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}
\theoremstyle{remark}
\newtheorem{remarkx}[theorem]{Remark}
\newenvironment{remark}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\remarkx}
  {\popQED\endremarkx}
  
\newenvironment{point}
  {\O~~}
  {}

\usepackage{thmtools}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package is for the long equal sign \xlongequal{}
\usepackage{extarrows}


% Page Formatting
\usepackage[
    paper=a3paper,
    inner=22mm,         % Inner margin
    outer=22mm,         % Outer margin
    bindingoffset=0mm, % Binding offset
    top=28mm,           % Top margin
    bottom=22mm,        % Bottom margin
    %showframe,         % show how the type block is set on the page
]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{.7em}


\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{topsep=0pt}

\usepackage{bm}

\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}
% \setlength{\parskip}{1em}
% \setlength{\parindent}{0em}
\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\CoV}{\operatorname{Co\mathbb{V}}}

% header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\small   \bfseries Homework}
\fancyhead[C]{\small   \bfseries Fall 2023}
\fancyhead[R]{\small   \bfseries Zhou}


\begin{document}

\begin{center}
  \text{\Large{Black-Litterman-Bayes, Kalman Filter, ICA
    }}

  {\text{Kaiwen Zhou}}
\end{center}
\vspace{2em}

\tableofcontents

\section*{Useful Theorem and Lemma}
\begin{theorem}
  \textbf{(Binomial Inverse Theorem - Woodbury Matrix Identity)} \label{theorem: Woodbury}If $\mathbf{A}$, $\mathbf{U}$,
  $\mathbf{B}$, $\mathbf{V}$ are matrices of sizes $n \times n, n \times k, k \times k, k \times n$,
  respectively, then
  \begin{enumerate}[label = (\alph*)]
    \item If $\mathbf{A}$ and $\mathbf{B}+\mathbf{B V A}^{-1}
            \mathbf{U B}$ are nonsingular:
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U B}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1}
          $$
    \item And (a) can be simplified to
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U}\left(\mathbf{B}^{-1}+\mathbf{V A}^{-1} \mathbf{U}\right)^{-1} \mathbf{V A}^{-1} .
          $$
  \end{enumerate}
\end{theorem}
\begin{solution}
  \begin{enumerate}[label = (\alph*)]
    \item Prove by verification.
          First notice that
          $$
            (\mathbf{A}+\mathbf{U B V}) \mathbf{A}^{-1} \mathbf{U B}=\mathbf{U B}+\mathbf{U B V A}^{-1} \mathbf{U B}=\mathbf{U}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right) .
          $$

          Now multiply the matrix we wish to invert by its alleged inverse
          $$
            \begin{aligned}
               & (\mathbf{A}+\mathbf{U B V})\left(\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U B}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1}\right)                   \\
               & =\mathbf{I}_n+\mathbf{U B V A}^{-1}-\mathbf{U}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1} \\
               & =\mathbf{I}_n+\mathbf{U B V A}^{-1}-\mathbf{U B V A}^{-1}                                                                                                                               \\
               & =\mathbf{I}_n
            \end{aligned}
          $$
          which verifies that it is the inverse.
    \item Since $\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}=\mathbf{B}\left(\mathbf{I}+\mathbf{V A}^{-1} \mathbf{U B}\right)$ is nonsingular, we must have $\mathbf{B}$ is invertible. Then the two $B$ terms flanking the quantity inverse in the right-hand side can be replaced with $\left(\mathbf{B}^{-1}\right)^{-1}$, which simplifies (a) to
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U}\left(\mathbf{B}^{-1}+\mathbf{V A}^{-1} \mathbf{U}\right)^{-1} \mathbf{V A}^{-1}
          $$
  \end{enumerate}
\end{solution}

\begin{lemma}\label{lemma: gaussian}
  If a multivariate normal random variable $\boldsymbol{\theta}$ has density $p(\boldsymbol{\theta})$ and
  $$
    -2 \log p(\boldsymbol{\theta})=\boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{\eta}^\top \boldsymbol{\theta}+(\text{terms without } \boldsymbol{\theta})
  $$
  then $\mathbb{V}[\boldsymbol{\theta}]=\boldsymbol{H}^{-1}$ and $\mathbb{E} [\boldsymbol{\theta}] =\boldsymbol{H}^{-1} \boldsymbol{\eta}$.
\end{lemma}
\begin{solution}
  For $\boldsymbol{H}$ symmetric, we have
  $$
    \boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{v}^\top \boldsymbol{H} \boldsymbol{\theta}=(\boldsymbol{\theta}-\boldsymbol{v})^\top \boldsymbol{H}(\boldsymbol{\theta}-\boldsymbol{v})-\boldsymbol{v}^\top \boldsymbol{H} \boldsymbol{v}
  $$
  Set $\boldsymbol{v}= \boldsymbol{H}^{-1} \boldsymbol{\eta}$ in the above equation, we obtain
  $$
    -2 \log p(\boldsymbol{\theta})=\boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{\eta}^\top \boldsymbol{\theta}+(\text{terms without } \boldsymbol{\theta}) = (\boldsymbol{\theta}-\boldsymbol{H}^{-1} \boldsymbol{\eta})^\top \boldsymbol{H}(\boldsymbol{\theta}-\boldsymbol{H}^{-1} \boldsymbol{\eta}) + (\text { terms without } \boldsymbol{\theta})
  $$
  Therefore, we must have $\mathbb{V}[\boldsymbol{\theta}]=\boldsymbol{H}^{-1}$ and $\mathbb{E} [\boldsymbol{\theta}] =\boldsymbol{H}^{-1} \boldsymbol{\eta}$.
\end{solution}

\begin{definition}\textbf{(Prior Predictive Distribution)}
  The prior predictive distribution is for predicting distribution for $\boldsymbol{x}$ BEFORE any sample of $\boldsymbol{x}$ has been
  gathered/observed. The only information we have at this stage is our belief about the
  prior, $\pi(\boldsymbol{\theta})$, and sampling distribution i.e. $p\left(\boldsymbol{x}
    \mid \boldsymbol{\theta}\right)$. Then, the prior predictive distribution is given by
  $$
    p(\boldsymbol{x}) =\int_{\Theta} p(\boldsymbol{x}, \boldsymbol{\theta}) d \boldsymbol{\theta} =\int_{\Theta} p(\boldsymbol{x} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d \boldsymbol{\theta}
  $$

  After the sample has been gathered, we obtain new information, i.e., the
  likelihood. Then, we can derive the posterior distribution
  $\boldsymbol{\theta} | \boldsymbol{x}_{\text{observed}}$ and use that to predict
  new values/distribution for $\boldsymbol{x}$ which is given by the posterior predictive
  distribution:
  $$
    p(\boldsymbol{x}_{\text{ new }} | \boldsymbol{x}_{\text{ observed}})
    =\int_{\Theta} p(\boldsymbol{x}, \boldsymbol{\theta} \mid \boldsymbol{x}_{\text{ observed}}) d \boldsymbol{\theta}
    =\int_{\Theta} p(\boldsymbol{x} \mid \boldsymbol{\theta}, \boldsymbol{x}_{\text{ observed}}) p(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{ observed}}) d \boldsymbol{\theta}
  $$
\end{definition}

\begin{lemma}\label{lemma: covariance matrix properties}
  If $X, Y, W$, and $V$ are real-valued random variables and $a, b, c, d$ are real-valued constants, then the following facts are a consequence of the definition of covariance:
  $$
    \begin{aligned}
      \operatorname{cov}(X, a)             & =0                                                                                                                   \\
      \operatorname{cov}(X, X)             & =\operatorname{var}(X)                                                                                               \\
      \operatorname{cov}(X, Y)             & =\operatorname{cov}(Y, X)                                                                                            \\
      \operatorname{cov}(a X, b Y)         & =a b \operatorname{cov}(X, Y)                                                                                        \\
      \operatorname{cov}(X+a, Y+b)         & =\operatorname{cov}(X, Y)                                                                                            \\
      \operatorname{cov}(a X+b Y, c W+d V) & =a c \operatorname{cov}(X, W)+a d \operatorname{cov}(X, V)+b c \operatorname{cov}(Y, W)+b d \operatorname{cov}(Y, V)
    \end{aligned}
  $$
\end{lemma}

\section{Topic: Black-Litterman-Bayes}
\begin{problem}
This question refers to the article ``On The Bayesian Interpretation Of
Black-Litterman'' \cite{Kolm2017}.
\begin{enumerate}[label=(\alph*)]
  \item Derive formulas (10)-(11) using the properties of the multivariate
        normal distribution in the slides ``Bayesian Modeling: Introduction''.
  \item \textbf{(Extra credit)} Derive formulas (22)-(26) using the same
        properties.
\end{enumerate}
\end{problem}
\begin{topic}
  \textbf{Classic Black-Litterman Model from Bayesian Perspective:}

  Suppose we have $\boldsymbol{r} \sim N(\boldsymbol{\theta}, \boldsymbol{\Sigma})$,
  and since Black and Litterman were motivated by
  the guiding principle that, in the absence of any sort of information/views
  which could constitute alpha over the benchmark, the optimization procedure
  should simply return the global CAPM equilibrium portfolio, with holdings
  denoted $\boldsymbol{h}_{e q}$. Hence in the absence of any views, and with
  prior mean equal to $\Pi$, the investor's model of the world is that
  \begin{align}
    \boldsymbol{r} \sim \mathcal{N}(\boldsymbol{\theta}, \boldsymbol{\Sigma}) \  \text { and } \  \boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\Pi}, \boldsymbol{C}) \quad \text{ where } \boldsymbol{r}, \boldsymbol{\theta},\boldsymbol{\Pi} \in \mathbb{R}^n, \boldsymbol{\Sigma}, \boldsymbol{C}\in \mathbb{R}^{n\times n}\label{eq:BLB returns}
  \end{align}
  A key aspect of the model is that the practitioner must also specify a level of
  uncertainty or ``error bar'' for each view, which is assumed to be an independent
  source of noise from the volatility already accounted for in a model such as
  $\boldsymbol{\Sigma}$. This is expressed as the following:
  \begin{align}
    \boldsymbol{P} \boldsymbol{\theta}=\boldsymbol{q}+\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{\Omega}), \quad \boldsymbol{\Omega}=\operatorname{diag}\left(\omega_1, \ldots, \omega_k\right), \omega_i > 0 \label{eq:BLB views}
  \end{align}
  where $\boldsymbol{P} \in \mathbb{R}^{k\times n}$, $\boldsymbol{\Omega} \in \mathbb{R}^{k\times k}$ and $\boldsymbol{q},\boldsymbol{\epsilon} \in \mathbb{R}^k$

  \begin{enumerate}[label=(\alph*)]
    \item \textbf{Solution:} In this question, we derive the mean $\boldsymbol{\nu}$ and covariance
          matrix $\boldsymbol{H}$ for the posterior.

          Since the posterior is proportional to the product of the likelihood and the
          prior, to simplify our computation, we neglect the constant coefficients of related probability density functions in
          our derivation.

          From \cref{eq:BLB views} and \cref{eq:BLB returns}, we have the likelihood function and the prior to be
          $$
            f(\boldsymbol{q} \mid \boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})^\top \boldsymbol{\Omega}^{-1}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})\right], \quad \pi(\boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{\theta} -\boldsymbol{\Pi})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{\theta} -\boldsymbol{\Pi})\right]
          $$
          Leveraging the Bayes's formula, we have $f(\boldsymbol{\theta}\mid \boldsymbol{q}) \propto f(\boldsymbol{q} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})$. It follows that (neglecting terms that do not contain $\boldsymbol{\theta}$)
          $$
            -2\log f(\boldsymbol{\theta}\mid \boldsymbol{q}) \propto (\boldsymbol{P} \boldsymbol{\theta} -\boldsymbol{q})^\top \boldsymbol{\Omega}^{-1}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})+(\boldsymbol{\theta}-\boldsymbol{\Pi})^\top \mathbf{C}^{-1}(\boldsymbol{\theta}-\boldsymbol{\Pi}) \xlongequal[\text{ drop terms without $\boldsymbol{\theta}$}]{\text{ completing the squares }}\boldsymbol{\theta}^\top\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \mathbf{P}+\boldsymbol{C}^{-1}\right] \boldsymbol{\theta}-2\left(\boldsymbol{q}^\top \boldsymbol{\Omega}^{-1} \mathbf{P}+\boldsymbol{\Pi}^\top \mathbf{C}^{-1}\right) \boldsymbol{\theta}
          $$
          By \cref{lemma: gaussian}, we obtain
          $$
            \boldsymbol{\theta}\mid \boldsymbol{q} \sim \mathcal{N}\left(\boldsymbol{\nu}, \boldsymbol{H}^{-1}\right), \quad \boldsymbol{\nu}=\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{P}+\boldsymbol{C}^{-1}\right]^{-1}\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{q}+\boldsymbol{C}^{-1} \boldsymbol{\Pi}\right] \text{ and } \boldsymbol{H}^{-1}=\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{P}+\boldsymbol{C}^{-1}\right]^{-1}
          $$
          as desired.

  \end{enumerate}
  \vspace*{0.8em}
  \hrule

  \textbf{APT Model:}

  Leveraging the powerful APT model (Roll \& Ross, 1980; Ross, 1976), the parameter vector $\boldsymbol{\theta}$ could be generalized to
  represent the means of unobservable latent factors in the APT
  model, which assumes a linear functional form:
  \begin{align}
    \boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon},
    \quad \boldsymbol{\epsilon}\sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}\right) \text{ and } \boldsymbol{D}=\operatorname{diag}\left(\sigma_1^2, \ldots, \sigma_n^2\right), \sigma_i^2>0,  \forall i
    \label{eq: APT model}
  \end{align}
  where $\boldsymbol{r}$ is an $n$-dimensional random vector containing the
  cross-section of returns in excess of the risk-free rate over some time interval
  $[t, t+1]$, and $\boldsymbol{X}$ is a (non-random) $n \times k$ matrix that is
  known before time $t$. The variable $\boldsymbol{f}$ in \cref{eq: APT model} denotes a
  $k$-dimensional latent random vector process, and
  information about the $\boldsymbol{f}$-process must be obtained via statistical
  inference. Specifically, we assume that the $\boldsymbol{f}$-process has finite
  first and second moments given by
  $$
    \mathbb{E}[\boldsymbol{f}]=\boldsymbol{\mu}_f \text { and } \mathbb{V}[\boldsymbol{f}]=\boldsymbol{F}
  $$
  In the Black-Litterman-Bayes model, we choose
  $$
    \boldsymbol{\theta} = \boldsymbol{\mu}_f,\quad
    \boldsymbol{f}\mid\boldsymbol{\theta} \sim \mathcal{N}\left(\boldsymbol{\theta}, \boldsymbol{F}\right)
  $$
  \textbf{Likelihood Function (Views):}

  For our understanding, let's assume we are considering two
  latent factors: value and momentum. Typically, a quantitative portfolio manager might have
  views on individual factors: (1) a view on the value premium, and (2) another
  view on the momentum premium. It would be atypical for portfolio managers to
  have views on a combination (e.g. linear) of factors. Hence to keep things simple but
  still useful, we take the views equation to be:
  $$
    \boldsymbol{\theta}=\boldsymbol{q}+\boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Omega}), \quad \boldsymbol{\Omega}=\operatorname{diag}\left(\omega_1^2, \ldots, \omega_k^2\right)
  $$
  then, $\boldsymbol{q} \mid \boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\theta},
    \boldsymbol{\Omega})$ and the corresponding likelihood function is therefore
  $$
    f(\boldsymbol{q} \mid \boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{q})^{\top} \boldsymbol{\Omega}^{-1}(\boldsymbol{\theta}-\boldsymbol{q})\right]=\prod_{i=1}^k \exp \left[-\frac{1}{2 \omega_i^2}\left(\theta_i-q_i\right)^2\right]
  $$

  \textbf{Prior:}

  What prior for $\boldsymbol{\theta}$ should we choose?
  First, let's set the prior $\pi(\boldsymbol{\theta})$ to be
  $$
    \pi(\boldsymbol{\theta}) \sim \mathcal{N}(\boldsymbol{\xi}, \boldsymbol{V})
  $$
  where $\boldsymbol{\xi} \in \mathbb{R}^k$ and $\boldsymbol{V} \in S_{++}^k$, the
  set of symmetric positive definite $k \times k$ matrices.

  Choosing a prior then amounts to choosing $\boldsymbol{\xi}$ and $\boldsymbol{V}$, and once
  a prior is chosen, we instantly obtain the prior predictive distribution $p(\boldsymbol{r})$ for $\boldsymbol{r}$ using the APT model and
  an associated prior (benchmark) portfolio with holdings $\boldsymbol{h}_B$
  where $\boldsymbol{h}_B$ maximizes expected utility of wealth, where the expectation
  is taken with respect to the a priori distribution on asset returns
  $$
    p(\boldsymbol{r})=\int p(\boldsymbol{r} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d \boldsymbol{\theta},
    \quad \boldsymbol{h}_B = \arg \max_{\boldsymbol{h}} \EE\left[u\left(\boldsymbol{h}^\top\boldsymbol{r}\right)\right]
    = \arg \max_{\boldsymbol{h}} \int u\left(\boldsymbol{h}^\top \boldsymbol{r}\right) p(\boldsymbol{r}) d \boldsymbol{r}
  $$
  Let's see what our prior (benchmark) portfolio will look like.

  Since $\boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon},
    \boldsymbol{\epsilon}\sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}\right)$ and $\boldsymbol{f}\mid\boldsymbol{\theta} \sim \mathcal{N}\left(\boldsymbol{\theta}, \boldsymbol{F}\right)$,
  we can equivalently rewrite it as $\boldsymbol{f} = \boldsymbol{\theta} + \boldsymbol{w}$
  where $\boldsymbol{w} \sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{F}\right)$, and $\boldsymbol{w}$ is independent of $\boldsymbol{\theta}$.
  Then, by properties of multivariate normal distribution, we have
  $$
    \boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon} =
    \boldsymbol{X} \boldsymbol{\theta}+\boldsymbol{X} \boldsymbol{w}+\boldsymbol{\epsilon}\quad
    \Longrightarrow \quad \boldsymbol{r}_{\pi} \sim \mathcal{N}\left(\boldsymbol{X}\boldsymbol{\xi}, \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
  $$
  The a priori optimal portfolio under CARA utility, $u(x) = -e^{-\lambda x}$, $\lambda \ne 0$, is then
  $$
    \boldsymbol{h}_B:=\boldsymbol{h}^*
    =\left(\lambda \mathbb{V}_\pi[\boldsymbol{r}]\right)^{-1} \mathbb{E}_\pi[\boldsymbol{r}]
    =\lambda^{-1} \left(\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)^{-1} \boldsymbol{X}\boldsymbol{\xi}
  $$

  \begin{remark}
    In the original paper, the author derived the prior predictive mean and variance as
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      \text{ and }
      \mathbb{V}_\pi[\boldsymbol{r}]=\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
    $$
    where $\boldsymbol{H}:=\boldsymbol{V}^{-1}+\boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{X}$ and $\boldsymbol{\Sigma}:=\boldsymbol{D}+\boldsymbol{X F} \boldsymbol{X}^\top$.

    We first note that due to his careless mistake, these results are wrong! The correct results are:
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      \quad \text{ and } \quad
      \mathbb{V}_\pi[\boldsymbol{r}]=\left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
    $$
    Then, substitute $\mathbf{A} = \boldsymbol{\Sigma}$, $\mathbf{U} = \mathbf{X}$, $\mathbf{V} = \mathbf{X}^\top$ and $\mathbf{B} = \mathbf{V}$ in the Woodbury identity (\cref{theorem: Woodbury}), we have
    $$
      \mathbb{V}_\pi[\boldsymbol{r}] = \left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
      =\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{\Sigma} = \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}
    $$
    It follows that
    $$
      \left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}\boldsymbol{\Sigma}^{-1} \boldsymbol{X}
      =\left(\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{\Sigma}\right)\boldsymbol{\Sigma}^{-1} \boldsymbol{X}
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{X} + \boldsymbol{X}
      = \boldsymbol{X}\boldsymbol{V}\left(\boldsymbol{V}^{-1} + \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{X}\right)
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{H}
    $$
    The mean $\mathbb{E}_\pi[\boldsymbol{r}]$ then follows as
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{H}\boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi} = \boldsymbol{X}\boldsymbol{\xi}
    $$
    Thus, we obtain
    $$
      \boldsymbol{r}_{\pi} \sim \mathcal{N}\left(\boldsymbol{X}\boldsymbol{\xi}, \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
    $$
    which is consistent with our results.
  \end{remark}
  \begin{enumerate}[label=(\alph*), start=2]
    \item \textbf{Solution:} In this question, we derive the posterior distribution for
          $\boldsymbol{r}\mid \boldsymbol{q}$ and the mean-variance optimal
          portfolio holdings.

          \textbf{Step 1: Calculate the posterior for $\boldsymbol{\theta} \mid  \boldsymbol{q}$.}

          Since $\pi(\boldsymbol{\theta})$ is normal and the likelihood
          $f(\boldsymbol{q} \mid \boldsymbol{\theta})$ is also normal, the prior
          is a conjugate prior with respect to the normal likelihood. Then, the
          posterior distribution $p(\boldsymbol{\theta} \mid \boldsymbol{q})$ is
          also normal. Using this relation and $p(\boldsymbol{\theta} \mid \boldsymbol{q}) \propto f(\boldsymbol{q} \mid \boldsymbol{\theta})\pi(\boldsymbol{\theta})$, we have (neglecting terms that do not contain $\boldsymbol{\theta}$)
          $$
            -2 \log p(\boldsymbol{\theta} \mid \boldsymbol{q})= (\boldsymbol{\theta}-\boldsymbol{q})^{\top} \boldsymbol{\Omega}^{-1}(\boldsymbol{\theta}-\boldsymbol{q})+(\boldsymbol{\theta}-\boldsymbol{\xi})^{\top} \mathbf{V}^{-1}(\boldsymbol{\theta}-\boldsymbol{\xi})
            =  \boldsymbol{\theta}^{\top}\left(\boldsymbol{\Omega}^{-1}+\mathbf{V}^{-1}\right) \boldsymbol{\theta}-2\left(\boldsymbol{q}^{\top} \boldsymbol{\Omega}^{-1}+\boldsymbol{\xi}^{\top} \mathbf{V}^{-1}\right) \boldsymbol{\theta}
            +(\text { terms without } \boldsymbol{\theta})
          $$
          Setting
          $\mathbf{H}=\left(\boldsymbol{\Omega}^{-1}+\mathbf{V}^{-1}\right)$ and
          $\boldsymbol{\eta}^{\top}=\left(\boldsymbol{q}^{\top}
            \boldsymbol{\Omega}^{-1}+\boldsymbol{\xi}^{\top}
            \mathbf{V}^{-1}\right)$ in \cref{lemma: gaussian}, we have
          $$
            \mathbb{V}[\boldsymbol{\theta} \mid \boldsymbol{q}]=\mathbf{H}^{-1}=\left( \boldsymbol{\Omega}^{-1}+ \boldsymbol{V}^{-1}\right)^{-1},
            \quad \mathbb{E}[\boldsymbol{\theta} \mid  \boldsymbol{q}]=\mathbf{H}^{-1} \boldsymbol{\eta} =\mathbb{V}[\boldsymbol{\theta} \mid \boldsymbol{q}]\left(\boldsymbol{V}^{-1} \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} q\right)
            =\left( \boldsymbol{V}^{-1}+ \boldsymbol{\Omega}^{-1}\right)^{-1}\left( \boldsymbol{V}^{-1}  \boldsymbol{\xi}+ \boldsymbol{\Omega}^{-1}  \boldsymbol{q}\right)
          $$
          Therefore, the posterior must satisfy
          $$
            \boldsymbol{\theta} \mid  \boldsymbol{q} \sim \mathcal{N}\left(\widetilde{\boldsymbol{\xi}}, \widetilde{ \boldsymbol{V}}\right)
          $$
          where
          $\widetilde{\boldsymbol{V}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}$
          and $\widetilde{\boldsymbol{\xi}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}\left(\boldsymbol{V}^{-1}
            \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} \boldsymbol{q}\right)$.

          \textbf{Step 2: Calculate the posterior for $\boldsymbol{r} \mid  \boldsymbol{q}$.}

          The a posteriori distribution of asset returns $\boldsymbol{r}$ (also
          called the posterior predictive density) is given by
          $$
            p(\boldsymbol{r} \mid \boldsymbol{q})=\int p(\boldsymbol{r} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{q}) d \boldsymbol{\theta}
          $$
          For this one, we can view $p(\boldsymbol{\theta} \mid \boldsymbol{q})$
          as the prior density for $\boldsymbol{\theta}$, then the situation
          here is the same as in deriving the prior (benchmark) portfolio.
          Hence, use the previous result and make the substitution
          $\boldsymbol{\xi} \rightarrow \widetilde{\boldsymbol{\xi}}$ and
          $\boldsymbol{V} \rightarrow \widetilde{\boldsymbol{V}}$, we obtain
          $$
            \boldsymbol{r} \mid \boldsymbol{q} \sim \mathcal{N}\left(\boldsymbol{X}\widetilde{\boldsymbol{\xi}}, \boldsymbol{X}\boldsymbol{\widetilde{V}}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
          $$

          \textbf{Step 3: Calculate the mean-variance optimal portfolio.}

          The posterior optimal portfolio under CARA utility is then
          $$
            \boldsymbol{h}^*
            =\left(\lambda \mathbb{V}[\boldsymbol{r}\mid \boldsymbol{q}]\right)^{-1} \mathbb{E}[\boldsymbol{r}\mid \boldsymbol{q}]
            =\lambda^{-1} \left(\boldsymbol{X}\boldsymbol{\widetilde{V}}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)^{-1} \boldsymbol{X}\boldsymbol{\widetilde{\xi}}
          $$
          where
          $\widetilde{\boldsymbol{V}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}$
          and $\widetilde{\boldsymbol{\xi}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}\left(\boldsymbol{V}^{-1}
            \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} \boldsymbol{q}\right)$.
  \end{enumerate}
\end{topic}



\section{Topic: Important Properties of the Kalman Filter}
\begin{topic}
  \textbf{State Space Models:}

  Consider the following discrete linear dynamical system, which also referred to as linear state space model:
  \begin{align}
    \begin{aligned}
       & \mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t} \\
       & \mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}
    \end{aligned}\quad \text{ where } \quad
    \mathbf{x}_{0} \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right),\quad
    \mathbf{w}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{Q}_{t}\right), \quad
    \mathbf{v}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{R}_{t}\right)
    \label{eq: state space model relation}
  \end{align}
  Additionally, the initial state $\mathbf{x}_{0}$ and the noise terms $\mathbf{w}_{t},
    \mathbf{v}_{t}$ are all assumed to be mutually independent.

  Sequences $\{\mathbf{x}_{\tau}\}_{\tau=0}^t$ and $\{\mathbf{y}_{\tau}\}_{\tau=0}^t$ are
  called the hidden/latent state and observation sequences, respectively.
  It follows that the first and second equations are referred to the state and observation
  equations, respectively.
  In a state space model the state sequences $\{\mathbf{x}_{\tau}\}_{\tau=0}^t$ are not observable;
  however, the observation sequences $\{\mathbf{y}_{\tau}\}_{\tau=0}^t$ are fully observable.


  \textbf{The Filtering Problem:}

  The filtering problem is to estimate
  $$
    \hat{\mathbf{x}}_{t}=\mathbb{E}\left(\mathbf{x}_{t} \mid \mathbf{Y}_{t}\right)
  $$
  where $\mathbf{Y}_{t}:=\left\{\mathbf{y}_{0}, \ldots, \mathbf{y}_{t}\right\}$.
  In other words, given the noisy observations,
  $\left\{\mathbf{y}_{\tau}\right\}_{\tau=0}^{t}$, we seek to estimate the
  expected state. And the optimal solution for our setup is given by the Kalman filter, also referred
  to as linear quadratic estimation (LQE).
  \vspace{0.8em}

  \hrule

  \textbf{Derive the Kalman Filter:}

  The derivation of the Kalman filter is by induction, showing that the
  observation and state sequences are conditionally Gaussian.

  \textbf{Base Case:} Since $\mathbf{x}_{0}$ is Gaussian, $\mathbf{x}_{1}, \mathbf{y}_{1}$ are also Gaussians.
  It follows that
  $$
    \begin{aligned}
       & \mathbf{x}_{1} \sim \mathcal{N}\left(\mathbf{F}_{1} \boldsymbol{\mu}_{0}, \mathbf{F}_{1} \mathbf{P}_{0} \mathbf{F}_{1}^\top+\mathbf{Q}_{1}\right)                                                                              \\
       & \mathbf{y}_{1} \sim \mathcal{N}\left(\mathbf{H}_{1} \mathbf{F}_{1} \boldsymbol{\mu}_{0}, \mathbf{H}_{1}\left(\mathbf{F}_{1} \mathbf{P}_{0} \mathbf{F}_{1}^\top+\mathbf{Q}_{1}\right) \mathbf{H}_{1}^\top+\mathbf{R}_{1}\right)
    \end{aligned}
  $$

  \textbf{Induction Step:} To perform the induction step, we assume that
  $$
    \mathbf{x}_{t-1} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{P}_{t-1 \mid t-1}\right)
  $$

  where the subscript $t \mid t^{\prime}$ to denote our belief about the state $\mathbf{x}_{t}$ given
  observations $\mathbf{Y}_{t^{\prime}}$. We immediately obtain from \cref{eq: state space model relation}

  $$
    \begin{aligned}
       & \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right)                                                                              \\
       & \mathbf{y}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{H}_{t} \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top+\mathbf{R}_{t}\right)
    \end{aligned}
  $$

  As $\mathbf{x}_{t}$, $\mathbf{w}_{t}$ and $\mathbf{v}_{t}$ are independent, the covariance of
  $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$ is given by
  $$
    \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{y}_{t}\right)
    =\operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}\right)
    =\operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{H}_{t} \mathbf{x}_{t}\right)
    + \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{v}_{t}\right)
    = \EE\left[\mathbf{x}_{t}\mathbf{x}_{t}^\top\mathbf{H}_{t}^\top\right]
    =\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
  $$

  \textbf{IS/Predict Step of the Kalman Filter:} From the distribution of $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, we introduce the notation
  $$
    \begin{aligned}
      \hat{\mathbf{x}}_{t \mid t-1} & :=\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}                              \\
      \mathbf{P}_{t \mid t-1}       & :=\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}
    \end{aligned}
  $$
  Then, we see that the joint distribution of
  $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$, conditioned on $\mathbf{Y}_{t-1}$, is given by the multivariate Gaussian
  $$
    \left[\begin{array}{l}
        \mathbf{x}_{t} \\
        \mathbf{y}_{t}
      \end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
        \hat{\mathbf{x}}_{t \mid t-1} \\
        \mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}
      \end{array}\right],\left[\begin{array}{cc}
        \mathbf{P}_{t \mid t-1}                & \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top                               \\
        \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} & \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t}
      \end{array}\right]\right)
  $$
  \textbf{IS/Update Step of the Kalman Filter:}
  Let us now assume $\mathbf{y}_{t}$ is observed.
  Applying the conditional formulas for the multivariate Gaussian we obtain
  $$
    \mathbf{x}_{t} \mid \mathbf{Y}_{t} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t \mid t}, \mathbf{P}_{t \mid t}\right)
  $$
  where
  \begin{align*}
    \hat{\mathbf{x}}_{t \mid t} :=\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right), \quad
    \mathbf{P}_{t \mid t} :=\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1},\quad
    \mathbf{K}_{t} :=\mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top \mathbf{S}_{t}^{-1}, \quad
    \mathbf{S}_{t} :=\mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t}
  \end{align*}
  Specifically, $\mathbf{K}_{t}$ is called the Kalman gain. Clearly, our belief about
  $\mathbf{x}_{t}$ given $\mathbf{Y}_{t}$ is a multivariate Gaussian distribution.

  \begin{remark}
    From the above filter formulas, it follows that we can apply the predict and
    update steps recursively as new observations arrive. This makes the Kalman
    filter ideal for online real-time processing of information.
  \end{remark}
\end{topic}

\begin{problem}
Choose three out of the four subproblems. It is optional to solve the other one
for extra credit.
\begin{enumerate}[label=(\alph*)]
  \item In class we derived the Kalman filter under the assumption that
        $\mathbb{E}\left[\mathbf{w}_t \mathbf{v}_t^\top\right]=\mathbf{0}$. Now,
        derive the Kalman filter under the assumption that
        $\mathbb{E}\left[\mathbf{w}_t \mathbf{v}_t^\top\right]=\mathbf{M}_t$.

        \begin{solution}
          We assume that
          $$
            \mathbf{x}_{t-1} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{P}_{t-1 \mid t-1}\right)
          $$
          where the subscript $t \mid t^{\prime}$ to denote our belief about the state $\mathbf{x}_{t}$ given
          observations $\mathbf{Y}_{t^{\prime}}$.

          \textbf{Find $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$ and $\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}$:}

          From \cref{eq: state space model relation}, we get $\mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t}$.
          Then, for $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, it's easy to deduce that
          $$
            \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right)
          $$

          Since $\mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t} =
            \mathbf{H}_{t} \mathbf{F}_{t}
            \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}$, we have
          $\EE\left[\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}\right] = \mathbf{H}_{t}
            \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}$. By \cref{lemma: covariance matrix properties}, we obtain
          \begin{align*}
            \VV\left[\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}\right]
            = \VV\left[\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right]
             & =  \operatorname*{Cov}\left(\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right) \\
            \mathbf{x}_{t-1} \text{ independent of } \mathbf{w}_{t} \text{ and } \mathbf{v}_{t} \Longrightarrow
             & =  \operatorname*{Cov}\left(\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}\right)
            + \operatorname*{Cov}\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}, \mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)                                                                                                   \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \EE\left[\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)^\top\right]                                                                                      \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \mathbf{H}_{t}\EE\left[\mathbf{w}_{t}\mathbf{w}_{t}^\top\right]\mathbf{H}_{t}^\top
            + \EE\left[\mathbf{v}_{t}\mathbf{v}_{t}^\top\right]
            + \EE\left[\mathbf{H}_{t}\mathbf{w}_{t}\mathbf{v}_{t}^\top\right]
            + \EE\left[\mathbf{v}_{t}\mathbf{w}_{t}^\top\mathbf{H}_{t}^\top\right]                                                                                                                                                       \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \mathbf{H}_{t}\mathbf{Q}_{t}\mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top                                                                                                                                                                                     \\
             & =\mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
          \end{align*}
          Therefore, we have
          \begin{align*}
            \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} & \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \\
            \mathbf{y}_{t} \mid \mathbf{Y}_{t-1}
                                                 & \sim
            \mathcal{N}\left(\mathbf{H}_{t} \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1},
            \mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top\right)
          \end{align*}

          \textbf{Find $\mathbf{x}_{t} \mid \mathbf{Y}_{t}$:}

          As $\mathbf{x}_{t-1}$, $\mathbf{w}_{t}$ and $\mathbf{x}_{t-1}$, $\mathbf{v}_{t}$ are independent, the covariance of
          $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$ is given by
          $$
            \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{y}_{t}\right)
            =\operatorname{Cov}\left(\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)
            =\operatorname{Cov}\left(\mathbf{F}_{t} \mathbf{x}_{t-1}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}\right)
            +\operatorname{Cov}\left(\mathbf{w}_{t}, \mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)
            =\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top + \mathbf{M}_{t}
          $$

          \textbf{- IS/Predict Step of the Kalman Filter:} From the distribution of $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, we introduce the notation
          $$
            \begin{aligned}
              \hat{\mathbf{x}}_{t \mid t-1} & :=\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}                              \\
              \mathbf{P}_{t \mid t-1}       & :=\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}
            \end{aligned}
          $$
          Then, we see that the joint distribution of
          $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$, conditioned on $\mathbf{Y}_{t-1}$, is given by the multivariate Gaussian
          $$
            \left[\begin{array}{l}
                \mathbf{x}_{t} \\
                \mathbf{y}_{t}
              \end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
                \hat{\mathbf{x}}_{t \mid t-1} \\
                \mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}
              \end{array}\right],\left[\begin{array}{cc}
                \mathbf{P}_{t \mid t-1}                                       & \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top  + \mathbf{M}_{t}                                                                                     \\
                \mathbf{H}_{t} \mathbf{P}_{t \mid t-1}  + \mathbf{M}_{t}^\top & \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t} + \mathbf{H}_{t}\mathbf{M}_{t} + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
              \end{array}\right]\right)
          $$
          \textbf{- IS/Update Step of the Kalman Filter:}
          Let us now assume $\mathbf{y}_{t}$ is observed.
          Applying the conditional formulas for the multivariate Gaussian we obtain
          $$
            \mathbf{x}_{t} \mid \mathbf{Y}_{t} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t \mid t}, \mathbf{P}_{t \mid t}\right)
          $$
          where
          \begin{align*}
            \hat{\mathbf{x}}_{t \mid t} :=\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right), ~~
            \mathbf{P}_{t \mid t} :=\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1} - \mathbf{K}_{t}\mathbf{M}_{t}^\top, ~~
            \mathbf{K}_{t} :=\mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top  + \mathbf{M}_{t}, ~
            \mathbf{S}_{t} :=\mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t} + \mathbf{H}_{t}\mathbf{M}_{t} + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
          \end{align*}
          Specifically, $\mathbf{K}_{t}$ is called the Kalman gain. Clearly, our belief about
          $\mathbf{x}_{t}$ given $\mathbf{Y}_{t}$ is a multivariate Gaussian distribution.
        \end{solution}

  \item \textbf{(The Kalman filter is the Best Predictor, among all (e.g. linear/non-linear), under Mean-Squared Error if noise is Gaussian)} Using the notation from class, let us denote the error between the true
        and estimated states by the random variable
        $\widetilde{\mathbf{x}}_t=\mathbf{x}_t-\hat{\mathbf{x}}_t$. We denote by
        $\mathbf{W}_t$ a known positive definite matrix. Show that the Kalman filter
        is the solution to the problem
        \begin{align}
          \min_{\hat{\mathbf{x}}_t} \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]\label{eq: kalman}
        \end{align}

        \begin{solution}
          Since $\mathbf{W}_t$ is a known positive definite matrix, by Cholesky
          decomposition, we have $\mathbf{W}_t =
            \mathbf{L}_t\mathbf{L}_t^\top$. Recall that the Kalman filter approximates
          $\hat{\mathbf{x}}_t = \hat{\mathbf{x}}_{t\mid t} = \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]$,
          and using the fact that $\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t
            \widetilde{\mathbf{x}}_t\in \mathbb{R}$, we can rewrite the loss
          function as
          \begin{align*}
            \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]
             & = \mathbb{E}\left[\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)^\top \mathbf{W}_t \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]
            + \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\right]                  \\
             & \quad + 2\cdot \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]       \\
             & =  \mathbb{E}\left[\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)^\top \mathbf{L}_t\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]
            + \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{L}_t\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\right] \\
            \phi(\hat{\mathbf{x}}_t) =\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t \Longrightarrow
             & \quad + 2\mathbb{E}\left\{\mathbb{E}\left[\phi(\hat{\mathbf{x}}_t)\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\mid \mathbf{Y}_t\right]\right\}                                             \\
             & =  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
            + \mathbb{E}\left[\|\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\|^2\right]
            + 2\mathbb{E}\left\{\mathbb{E}\left[\phi(\hat{\mathbf{x}}_t)\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]\right\}                                      \\
             & =  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
            + \mathbb{E}\left[\|\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\|^2\right]                                                                                            \\
             & \ge  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
          \end{align*}
          where the equality can be attained when $\hat{\mathbf{x}}_t = \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]$.

          Therefore, the Kalman filter is the solution to the problem $\min_{\hat{\mathbf{x}}_t} \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]$.
        \end{solution}
        \item \textbf{(The Kalman filter is the Best LINEAR Predictor under Mean-Squared Error if noise is NOT Gaussian (Non-linear Predictors could be better.))} Now
        let us assume that $\mathbf{w}_t$ and $\mathbf{v}_t$ have zero mean, are
        uncorrelated with covariance matrices $\mathbf{Q}_t$ and $\mathbf{R}_t$,
        respectively (but they are no longer Gaussian). Show that the Kalman filter is
        the best linear solution to \cref{eq: kalman}.

        \begin{solution}
          The best linear unbiased estimator (BLUE) of the vector $\beta$ of
          parameters $\beta_j$ is one with the smallest mean squared error for
          every vector $\lambda$ of linear combination parameters. This is
          equivalent to the condition that
          $$
            \operatorname{Var}(\tilde{\beta})-\operatorname{Var}(\widehat{\beta})
          $$
          is a positive semi-definite matrix for every other linear unbiased estimator
          $\tilde{\beta}$.

        \end{solution}
  \item In class we derived the Kalman filter under the assumption that
        $\mathbf{w}_t$ and $\mathbf{v}_t$ are uncolored (i.e. each is serially
        uncorrelated).
        \begin{enumerate}[label=(\roman*)]
          \item Derive the Kalman filter under the assumption that
                $\mathbf{w}_t$ is a VAR(1) process with known system matrix.
          \item Derive the Kalman filter under the assumption that
                $\mathbf{v}_t$ is a $\operatorname{VAR}(1)$ process with known
                system matrix.
        \end{enumerate}
        Hint: For (d), both (i) and (ii) can be solved by properly augmenting
        the state equations. Can you find a solution to (ii) where one does not
        have to augment the state? Is it possible to do so for (i) - why, or why
        not?

        \begin{solution}

        \end{solution}
\end{enumerate}

\end{problem}


\section{Topic: Kalman filter's Application on Financial Problems --- Pairs Trading}
\begin{problem}
In this question we consider a basic pairs trading strategy between two stocks
with prices $p_t^A$ and $p_t^B$ at time $t$. We denote the spread between them
by $s_t:=\log \left(p_t^A\right)-\log \left(p_t^B\right)$ and assume the spread
follows an Ornstein-Uhlenbeck process
\begin{align}
  d s_t=\kappa\left(\theta-s_t\right) d t+\sigma d B_t \label{eq: OU process}
\end{align}
where $d B_t$ is a standard Brownian motion. In other words, the spread reverts
to its mean $\theta \in \mathbb{R}$ at the speed $\kappa \in \mathbb{R}_{+}$and
volatility $\sigma \in \mathbb{R}_{+}$.
\begin{enumerate}[label=(\alph*)]
  \item Show that the discrete time solution of \cref{eq: OU process} is Markovian, that is
        $$
          s_n=\mathbb{E}\left[s_n \mid s_{n-1}\right]+\varepsilon_n
        $$
        where $n=1,2, \ldots$, and $\varepsilon_n$ is a random process with zero
        mean and variance equal to $\sigma_{\varepsilon,
            n}^2=\mathbb{V}\left[s_n \mid s_{n-1}\right]$. 
            
            Hint: You can derive the discrete solution explicitly.

        \begin{solution}
          Set $f(t, x) = e^{\kappa t}x$ and apply the It\^{o}'s lemma, we have
          $de^{e^{\kappa t}s_t} = \kappa e^{\kappa t}s_t dt + e^{\kappa t} ds_t$. Pluging in \cref{eq: OU process}, we obtain
          $$
          d s_t + \kappa s_t d t = \frac{d e^{\kappa t}s_t}{e^{\kappa t}} = \kappa \theta dt+\sigma d B_t 
          \Longrightarrow
          e^{\kappa t}s_t - s_0 = \int_{0}^t e^{\kappa s}\kappa \theta ds + \int_{0}^t e^{\kappa s}\sigma d B_s 
          \Longrightarrow
          s_t = e^{-\kappa t}s_0 + \theta \left(1 - e^{-\kappa t}\right) + \int_{0}^t e^{\kappa(s-t)}\sigma d B_s, \forall t>0
          $$
          To extract $s_{n-1}$ out of $s_n$, we can rewrite $s_n$ as:
          \begin{align*}
          s_n = e^{-\kappa n}s_0 + \theta \left(1 - e^{-\kappa n}\right) + \int_{0}^n e^{\kappa(s-n)}\sigma d B_s 
          &= \theta - e^{-\kappa}\theta + e^{-\kappa} \cdot \left(e^{-\kappa (n-1)}s_0 + \theta \left(1 - e^{-\kappa (n-1)}\right) + e^{-\kappa (n-1)} \left[\int_{0}^{n-1} e^{\kappa s}\sigma d B_s + \int_{n-1}^n e^{\kappa s}\sigma d B_s\right]\right)\\
          &= \theta - e^{-\kappa}\theta + e^{-\kappa} s_{n-1} + e^{-\kappa n}\int_{n-1}^n e^{\kappa s}\sigma d B_s
        \end{align*}
        \end{solution}
  \item Propose a methodology for updating the parameters $\theta, \kappa$ using
        the Kalman filter and describe how you would use it to trade the stock pair.

        \begin{solution}
          
        \end{solution}
  \item Test your methodology from (b) on simulated data. In particular, (i)
        simulate (2) from known parameters $\theta, \kappa$ and $\sigma$, and then
        (ii) use the Kalman filter to recover them. You do not need to implement the
        Kalman filter from scratch; you are welcome to use a Kalman implementation
        from a Python package such as pykalman. How do you obtain a good estimate of
        $\sigma$ ?

        \begin{solution}
          
        \end{solution}
  \item Repeat the same experiment from (c), but this time simulate (2) first
        with $\kappa$ having the same value as above and then suddenly changing it to
        another value such that the half-life of the spread is $50 \%$ of its original
        value. How long does it take the Kalman filter to adjust? Can you make
        adjustment to your filter in order to speed up the time it takes the Kalman
        filter to adjust?

        \begin{solution}
          
        \end{solution}
\end{enumerate}
Hint: For (c) and (d), think about how you are going to demonstrate the results
using appropriate graphs, etc.
\end{problem}

\section{Topic: Kalman filter's Application on Financial Problems --- Index Tracking Portfolios}

\begin{problem}
It is common in portfolio management to build so-called (index) tracking
portfolios. Let us assume we are observing the return of the S\&P 500 benchmark
index, $r_{b, t}$. Now, let us pick a subset of 50 stocks from the constituents
of this index. We will use these stocks to build a tracking portfolio for the
index. For example, this could be the 50 companies in the index with the largest
market cap. We denote the returns of these 50 stock by $\mathbf{r}_t \in
  \mathbb{R}^{50}$. The goal of finding a tracking portfolio is to find a dynamic
trading strategy of the 50 stocks such that $\boldsymbol{\beta}_t^\top
  \mathbf{r}_t \approx r_{b, t}$, where $\boldsymbol{\beta}_t$ denotes the
holdings of the tracking portfolio.
\begin{enumerate}[label=(\alph*)]
  \item In this part, we assume that the covariance matrix of returns of the
        stocks in the $\mathrm{S} \& \mathrm{P} 500, \boldsymbol{\Sigma}$, is given
        and constant through time. Find the portfolio of these 50 stocks that
        minimizes the tracking error to $r_{b, t}$, i.e. find the solution to
        $$
          \boldsymbol{\beta}_t^*=\operatorname{argmin}_{\boldsymbol{\beta}_t} \sqrt{\mathbb{V}\left[r_{b, t}-\boldsymbol{\beta}_t^\top \mathbf{r}_t\right]} .
        $$

        What specific property does $\boldsymbol{\beta}_t$ have here?
  \item In this part, we no longer assume that covariances amongst stocks are
        time invariant. Propose a solution to minimizing the tracking error using the
        Kalman filter.
  \item Download daily market data and create an example that illustrates your
        methodology. How does the tracking portfolio of the Kalman filter perform?
\end{enumerate}
Hint: For (c), compare your Kalman filter to a solution based on (a) where the
covariance matrix is estimated on rolling windows. Can you match the performance
of the Kalman filter with the simpler methodology in (a) by appropriately
choosing the length of the rolling window?
\end{problem}


\section{Topic: Another Interpretation for Independent Component Analysis (ICA)}
\begin{problem}
In class we considered the principally-independent component analysis method
which essentially was the truncated rank- $K$ SVD of a matrix X followed by an
ICA rotation of the left singular components:
$$
  \mathbf{X} \simeq \mathbf{U S V}^\top=\mathbf{U}_I \mathbf{S}_I \mathbf{V}_I
$$
where $\mathbf{U}_I=\mathbf{U} \mathbf{A}_I$ with
$\mathbf{A}_I=\underset{\mathbf{A}, \mathbf{A}^\top
    \mathbf{A}=I_K}{\operatorname{argmax}}\left|k_{\ell}(\mathbf{U A})\right|,
  k_{\ell}(\mathbf{G})$ being any centered cumulant of order $\ell \geq 3$ which
for all practical purposes can be considered a non-linear (activation)

function applied to each of the entries of $\mathbf{G}$. Furthermore the matrix
$\mathbf{V}_I$ was defined as $\mathbf{V}_I^\top:=\mathbf{D}^{-1}
  \mathbf{S}^{-1} \mathbf{A}_I \mathbf{S} \mathbf{V}^\top$ where $\mathbf{D}$
was chosen so that $\mathbf{V}_I$ has unimodular columns.
\begin{enumerate}[label=(\alph*)]
  \item Show that $\mathbf{D}$ is a diagonal matrix
  \item Show that $\mathbf{S}_I=\mathbf{S D}$ is diagonal such that
        $\operatorname{Tr}\left(\mathbf{S}_I^2\right)=\operatorname{Tr}\left(\mathbf{S}^2\right)$.
  \item Show that the method can be derived as the limit, $\lambda^2 \rightarrow
          0$, of the optimization
        $$
          \mathbf{U}_I, \mathbf{S}_I, \mathbf{V}_I=\underset{\mathbf{P}, \mathbf{Q}: \mathbf{P}^\top \mathbf{P}=\operatorname{diag}\left(\mathbf{Q}^\top \mathbf{Q}\right)=\mathbf{I}_k}{\operatorname{argmin}}\left\|\mathbf{X}-\mathbf{P R Q}^\top\right\|_F-\lambda^2\left|k_{\ell}(\mathbf{P})\right| .
        $$
  \item Show that an alternative objective function achieving the same result is
        $$
          \mathbf{U}_I, \mathbf{S}_I, \mathbf{V}_I=\underset{\mathbf{P}, \mathbf{Q}: \mathbf{P}^\top \mathbf{P}=\operatorname{diag}\left(\mathbf{Q}^\top \mathbf{Q}\right)=\mathbf{I}_k}{\operatorname{argmin}}\left\|\mathbf{X}-\mathbf{P R Q}^\top\right\|_F-\lambda^2 J(\mathbf{P}),
        $$
        where $J[\mathbf{x}]:=H\left[\mathbf{x}_{\text {gauss
          }}\right]-H[\mathbf{x}]$ is the negentropy and $J(\mathbf{P})$ is the
        sum of the negentropies of all the columns of $\mathbf{P}$.
\end{enumerate}
\end{problem}














\newpage
\begin{thebibliography}{1}

  \bibitem{Kolm2017}
  Kolm, Petter N. and Ritter, Gordon, On the Bayesian Interpretation of
  Black-Litterman (October 16, 2016). European Journal of Operational Research,
  Volume 258, Issue 2, 16 April 2017, Pages 564-572, Available at SSRN:
  \href{https://ssrn.com/abstract=2853158}{https://ssrn.com/abstract=2853158}.

\end{thebibliography}






\end{document}

