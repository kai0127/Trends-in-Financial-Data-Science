\documentclass[11pt]{article}

\usepackage{amsmath,amsthm,amssymb}

%%%%% Matrix stretcher
% use it as:
%\begin{pmatrix}[1.5]
% ...
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}


%%%%%%%%%%%%% Colors %%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}

\definecolor{C0}{HTML}{1d1d1d}
\definecolor{C1}{HTML}{1e3668}
\definecolor{C2}{HTML}{199d8b}
\definecolor{C3}{HTML}{d52f4c}
\definecolor{C4}{HTML}{5ab2d6}
\definecolor{C5}{HTML}{ffb268}
\definecolor{C6}{HTML}{ff7300} % for commenting - {fire orange}dd571c
\definecolor{C7}{HTML}{777b7e} % for remarks - {steel grey}
\color{C0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%% Fonts %%%%%%%%%%%%% 
%\usepackage{fontspec}
\usepackage[no-math]{fontspec} % for text

\emergencystretch=8pt
\hyphenpenalty=1000 % default 50
\tolerance=800      % default 200
%\righthyphenmin=4
%\lefthyphenmin=4

%%% Text Font: Vollkorn + Math Font: Latin Modern (default) %%%
\setmainfont{Vollkorn}[
UprightFont = Vollkorn-Regular,
ItalicFont =Vollkorn-Italic, 
BoldItalicFont={Vollkorn-BoldItalic},
BoldFont = Vollkorn-Bold,
RawFeature=+lnum,
WordSpace=1.7,
] 

%%% We need this for math font packages other than latin modern %%%
% \usepackage{unicode-math}        % for math

%%% Text Font: Palatino + Math Font: Asana-Math %%%
%\setmainfont{Palatino}[
%BoldFont = Palatino-Bold,
%ItalicFont = Palatino-Italic,
%BoldItalicFont={Palatino-BoldItalic},
%RawFeature=+lnum,
%WordSpace=1.7,
%]
%\setmathfont{asana-math}

%%% Text Font: Arno Pro + Math Font: Minion Pro %%%
%\setmainfont{Arno Pro}[
%UprightFont = *-Regular,
%ItalicFont = Vollkorn-Italic, 
%BoldItalicFont={*-BoldItalic},
%BoldFont = *-Bold,
%RawFeature=+lnum,
%WordSpace=1.7,
%Scale= 1.1
%] 
% Minion Pro is too expensive

%%% Math Fonts %%%
%\setmathfont{Vollkorn}
%\setmathfont{Latin Modern Math}
%\setmathfont{TeX Gyre Pagella Math}
%\setmathfont{TeX Gyre Termes Math}
%\setmathfont{TeX Gyre DejaVu Math}
%\setmathfont[Scale=MatchLowercase]{DejaVu Math TeX Gyre}
%\setmathfont{XITS Math}
%\setmathfont{Libertinus Math}
%\setmathfont[Scale=MatchUppercase]{Asana Math}
%\setmathfont{STIX Two Math}

%\usepackage{kpfonts-otf}
%\setmathfont{KpMath-Regular.otf}[version=regular]
%\setmathfont{KpMath-Bold.otf}[version=bold]
%\setmathfont{KpMath-Semibold.otf}[version=semibold]
%\setmathfont{KpMath-Sans.otf}[version=sans]
%\setmathfont{KpMath-Light.otf}[version=light]


%%% CJK Fonts %%%
\usepackage[scale=.78]{luatexja-fontspec}
\setmainjfont{BabelStone Han}[AutoFakeBold]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package simplifies the insertion of external multi-page PDF or PS documents.
\usepackage{pdfpages}

% cref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=C4,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\usepackage[nameinlink,noabbrev,capitalize]{cleveref}
% \crefname{ineq}{}{}
% \crefname{equation}{}{}
% \creflabelformat{ineq}{#2{\textup{(1)}}#3}
% \creflabelformat{equation}{#2\textup{(#1)}#3}

%%%%%%%%%%%%% Environments %%%%%%%%%%%%%%%%
%amsthm has three separate predefined styles:	
%
%\theoremstyle{plain} is the default. it sets the text in italic and adds extra space above and below the \newtheorems listed below it in the input. it is recommended for theorems, corollaries, lemmas, propositions, conjectures, criteria, and (possibly; depends on the subject area) algorithms.
%
%\theoremstyle{definition} adds extra space above and below, but sets the text in roman. it is recommended for definitions, conditions, problems, and examples; i've alse seen it used for exercises.
%
%\theoremstyle{remark} is set in roman, with no additional space above or below. it is recommended for remarks, notes, notation, claims, summaries, acknowledgments, cases, and conclusions.

%%%  theorem-like environment %%%
\theoremstyle{plain} % default theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\newtheorem{definition}[theorem]{Definition}

%%% definition-like environment %%%
%\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}


%%% framed package is great %%%
\usepackage{framed}
\newenvironment{solution}
{\color{C2}\normalfont\begin{framed}\begingroup\textbf{Solution:} }
  {\endgroup\end{framed}}

\newenvironment{topic}
  {\color{C2}\normalfont\begin{framed}\begingroup}
    {\endgroup\end{framed}}


\newtheoremstyle{remark}% name of the style to be used
  {}% measure of space to leave above the theorem. E.g.: 3pt
  {}% measure of space to leave below the theorem. E.g.: 3pt
  {\color{C3}}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\color{C3}\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}
\theoremstyle{remark}
\newtheorem{remarkx}[theorem]{Remark}
\newenvironment{remark}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\remarkx}
  {\popQED\endremarkx}
  
\newenvironment{point}
  {\O~~}
  {}

\usepackage{thmtools}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package is for the long equal sign \xlongequal{}
\usepackage{extarrows}


% Page Formatting
\usepackage[
    paper=a3paper,
    inner=22mm,         % Inner margin
    outer=22mm,         % Outer margin
    bindingoffset=0mm, % Binding offset
    top=28mm,           % Top margin
    bottom=22mm,        % Bottom margin
    %showframe,         % show how the type block is set on the page
]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{.7em}


\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{topsep=0pt}

\usepackage{bm}

\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}
% \setlength{\parskip}{1em}
% \setlength{\parindent}{0em}
\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\CoV}{\operatorname{Co\mathbb{V}}}

% header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\small   \bfseries Homework}
\fancyhead[C]{\small   \bfseries Fall 2023}
\fancyhead[R]{\small   \bfseries Zhou}


\begin{document}

\begin{center}
  \text{\Large{Black-Litterman-Bayes, Kalman Filter, ICA
    }}

  {\text{Kaiwen Zhou, Youran Pan, Erding Liao}}
\end{center}
\vspace{2em}

\tableofcontents

\section*{Useful Theorem and Lemma}
\begin{theorem}
  \textbf{(Binomial Inverse Theorem - Woodbury Matrix Identity)} \label{theorem: Woodbury}If $\mathbf{A}$, $\mathbf{U}$,
  $\mathbf{B}$, $\mathbf{V}$ are matrices of sizes $n \times n, n \times k, k \times k, k \times n$,
  respectively, then
  \begin{enumerate}[label = (\alph*)]
    \item If $\mathbf{A}$ and $\mathbf{B}+\mathbf{B V A}^{-1}
            \mathbf{U B}$ are nonsingular:
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U B}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1}
          $$
    \item And (a) can be simplified to
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U}\left(\mathbf{B}^{-1}+\mathbf{V A}^{-1} \mathbf{U}\right)^{-1} \mathbf{V A}^{-1} .
          $$
  \end{enumerate}
\end{theorem}
\begin{solution}
  \begin{enumerate}[label = (\alph*)]
    \item Prove by verification.
          First notice that
          $$
            (\mathbf{A}+\mathbf{U B V}) \mathbf{A}^{-1} \mathbf{U B}=\mathbf{U B}+\mathbf{U B V A}^{-1} \mathbf{U B}=\mathbf{U}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right) .
          $$

          Now multiply the matrix we wish to invert by its alleged inverse
          $$
            \begin{aligned}
               & (\mathbf{A}+\mathbf{U B V})\left(\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U B}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1}\right)                   \\
               & =\mathbf{I}_n+\mathbf{U B V A}^{-1}-\mathbf{U}\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)\left(\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}\right)^{-1} \mathbf{B V A}^{-1} \\
               & =\mathbf{I}_n+\mathbf{U B V A}^{-1}-\mathbf{U B V A}^{-1}                                                                                                                               \\
               & =\mathbf{I}_n
            \end{aligned}
          $$
          which verifies that it is the inverse.
    \item Since $\mathbf{B}+\mathbf{B V A}^{-1} \mathbf{U B}=\mathbf{B}\left(\mathbf{I}+\mathbf{V A}^{-1} \mathbf{U B}\right)$ is nonsingular, we must have $\mathbf{B}$ is invertible. Then the two $B$ terms flanking the quantity inverse in the right-hand side can be replaced with $\left(\mathbf{B}^{-1}\right)^{-1}$, which simplifies (a) to
          $$
            (\mathbf{A}+\mathbf{U B V})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{U}\left(\mathbf{B}^{-1}+\mathbf{V A}^{-1} \mathbf{U}\right)^{-1} \mathbf{V A}^{-1}
          $$
  \end{enumerate}
\end{solution}

\begin{lemma}\label{lemma: gaussian}
  If a multivariate normal random variable $\boldsymbol{\theta}$ has density $p(\boldsymbol{\theta})$ and
  $$
    -2 \log p(\boldsymbol{\theta})=\boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{\eta}^\top \boldsymbol{\theta}+(\text{terms without } \boldsymbol{\theta})
  $$
  then $\mathbb{V}[\boldsymbol{\theta}]=\boldsymbol{H}^{-1}$ and $\mathbb{E} [\boldsymbol{\theta}] =\boldsymbol{H}^{-1} \boldsymbol{\eta}$.
\end{lemma}
\begin{solution}
  For $\boldsymbol{H}$ symmetric, we have
  $$
    \boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{v}^\top \boldsymbol{H} \boldsymbol{\theta}=(\boldsymbol{\theta}-\boldsymbol{v})^\top \boldsymbol{H}(\boldsymbol{\theta}-\boldsymbol{v})-\boldsymbol{v}^\top \boldsymbol{H} \boldsymbol{v}
  $$
  Set $\boldsymbol{v}= \boldsymbol{H}^{-1} \boldsymbol{\eta}$ in the above equation, we obtain
  $$
    -2 \log p(\boldsymbol{\theta})=\boldsymbol{\theta}^\top \boldsymbol{H} \boldsymbol{\theta}-2 \boldsymbol{\eta}^\top \boldsymbol{\theta}+(\text{terms without } \boldsymbol{\theta}) = (\boldsymbol{\theta}-\boldsymbol{H}^{-1} \boldsymbol{\eta})^\top \boldsymbol{H}(\boldsymbol{\theta}-\boldsymbol{H}^{-1} \boldsymbol{\eta}) + (\text { terms without } \boldsymbol{\theta})
  $$
  Therefore, we must have $\mathbb{V}[\boldsymbol{\theta}]=\boldsymbol{H}^{-1}$ and $\mathbb{E} [\boldsymbol{\theta}] =\boldsymbol{H}^{-1} \boldsymbol{\eta}$.
\end{solution}

\begin{definition}\textbf{(Prior Predictive Distribution)}
  The prior predictive distribution is for predicting distribution for $\boldsymbol{x}$ BEFORE any sample of $\boldsymbol{x}$ has been
  gathered/observed. The only information we have at this stage is our belief about the
  prior, $\pi(\boldsymbol{\theta})$, and sampling distribution i.e. $p\left(\boldsymbol{x}
    \mid \boldsymbol{\theta}\right)$. Then, the prior predictive distribution is given by
  $$
    p(\boldsymbol{x}) =\int_{\Theta} p(\boldsymbol{x}, \boldsymbol{\theta}) d \boldsymbol{\theta} =\int_{\Theta} p(\boldsymbol{x} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d \boldsymbol{\theta}
  $$

  After the sample has been gathered, we obtain new information, i.e., the
  likelihood. Then, we can derive the posterior distribution
  $\boldsymbol{\theta} | \boldsymbol{x}_{\text{observed}}$ and use that to predict
  new values/distribution for $\boldsymbol{x}$ which is given by the posterior predictive
  distribution:
  $$
    p(\boldsymbol{x}_{\text{ new }} | \boldsymbol{x}_{\text{ observed}})
    =\int_{\Theta} p(\boldsymbol{x}, \boldsymbol{\theta} \mid \boldsymbol{x}_{\text{ observed}}) d \boldsymbol{\theta}
    =\int_{\Theta} p(\boldsymbol{x} \mid \boldsymbol{\theta}, \boldsymbol{x}_{\text{ observed}}) p(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{ observed}}) d \boldsymbol{\theta}
  $$
\end{definition}

\begin{lemma}\label{lemma: covariance matrix properties}
  If $X, Y, W$, and $V$ are real-valued random variables and $a, b, c, d$ are real-valued constants, then the following facts are a consequence of the definition of covariance:
  $$
    \begin{aligned}
      \operatorname{cov}(X, a)             & =0                                                                                                                   \\
      \operatorname{cov}(X, X)             & =\operatorname{var}(X)                                                                                               \\
      \operatorname{cov}(X, Y)             & =\operatorname{cov}(Y, X)                                                                                            \\
      \operatorname{cov}(a X, b Y)         & =a b \operatorname{cov}(X, Y)                                                                                        \\
      \operatorname{cov}(X+a, Y+b)         & =\operatorname{cov}(X, Y)                                                                                            \\
      \operatorname{cov}(a X+b Y, c W+d V) & =a c \operatorname{cov}(X, W)+a d \operatorname{cov}(X, V)+b c \operatorname{cov}(Y, W)+b d \operatorname{cov}(Y, V)
    \end{aligned}
  $$
\end{lemma}

\section{Topic: Black-Litterman-Bayes}
\begin{problem}
This question refers to the article ``On The Bayesian Interpretation Of
Black-Litterman'' \cite{Kolm2017}.
\begin{enumerate}[label=(\alph*)]
  \item Derive formulas (10)-(11) using the properties of the multivariate
        normal distribution in the slides ``Bayesian Modeling: Introduction''.
  \item \textbf{(Extra credit)} Derive formulas (22)-(26) using the same
        properties.
\end{enumerate}
\end{problem}
\begin{topic}
  \textbf{Classic Black-Litterman Model from Bayesian Perspective:}

  Suppose we have $\boldsymbol{r} \sim N(\boldsymbol{\theta}, \boldsymbol{\Sigma})$,
  and since Black and Litterman were motivated by
  the guiding principle that, in the absence of any sort of information/views
  which could constitute alpha over the benchmark, the optimization procedure
  should simply return the global CAPM equilibrium portfolio, with holdings
  denoted $\boldsymbol{h}_{e q}$. Hence in the absence of any views, and with
  prior mean equal to $\Pi$, the investor's model of the world is that
  \begin{align}
    \boldsymbol{r} \sim \mathcal{N}(\boldsymbol{\theta}, \boldsymbol{\Sigma}) \  \text { and } \  \boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\Pi}, \boldsymbol{C}) \quad \text{ where } \boldsymbol{r}, \boldsymbol{\theta},\boldsymbol{\Pi} \in \mathbb{R}^n, \boldsymbol{\Sigma}, \boldsymbol{C}\in \mathbb{R}^{n\times n}\label{eq:BLB returns}
  \end{align}
  A key aspect of the model is that the practitioner must also specify a level of
  uncertainty or ``error bar'' for each view, which is assumed to be an independent
  source of noise from the volatility already accounted for in a model such as
  $\boldsymbol{\Sigma}$. This is expressed as the following:
  \begin{align}
    \boldsymbol{P} \boldsymbol{\theta}=\boldsymbol{q}+\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{\Omega}), \quad \boldsymbol{\Omega}=\operatorname{diag}\left(\omega_1, \ldots, \omega_k\right), \omega_i > 0 \label{eq:BLB views}
  \end{align}
  where $\boldsymbol{P} \in \mathbb{R}^{k\times n}$, $\boldsymbol{\Omega} \in \mathbb{R}^{k\times k}$ and $\boldsymbol{q},\boldsymbol{\epsilon} \in \mathbb{R}^k$

  \begin{enumerate}[label=(\alph*)]
    \item \textbf{Solution:} In this question, we derive the mean $\boldsymbol{\nu}$ and covariance
          matrix $\boldsymbol{H}$ for the posterior.

          Since the posterior is proportional to the product of the likelihood and the
          prior, to simplify our computation, we neglect the constant coefficients of related probability density functions in
          our derivation.

          From \cref{eq:BLB views} and \cref{eq:BLB returns}, we have the likelihood function and the prior to be
          $$
            f(\boldsymbol{q} \mid \boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})^\top \boldsymbol{\Omega}^{-1}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})\right], \quad \pi(\boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{\theta} -\boldsymbol{\Pi})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{\theta} -\boldsymbol{\Pi})\right]
          $$
          Leveraging the Bayes's formula, we have $f(\boldsymbol{\theta}\mid \boldsymbol{q}) \propto f(\boldsymbol{q} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})$. It follows that (neglecting terms that do not contain $\boldsymbol{\theta}$)
          $$
            -2\log f(\boldsymbol{\theta}\mid \boldsymbol{q}) \propto (\boldsymbol{P} \boldsymbol{\theta} -\boldsymbol{q})^\top \boldsymbol{\Omega}^{-1}(\boldsymbol{P} \boldsymbol{\theta}-\boldsymbol{q})+(\boldsymbol{\theta}-\boldsymbol{\Pi})^\top \mathbf{C}^{-1}(\boldsymbol{\theta}-\boldsymbol{\Pi}) \xlongequal[\text{ drop terms without $\boldsymbol{\theta}$}]{\text{ completing the squares }}\boldsymbol{\theta}^\top\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \mathbf{P}+\boldsymbol{C}^{-1}\right] \boldsymbol{\theta}-2\left(\boldsymbol{q}^\top \boldsymbol{\Omega}^{-1} \mathbf{P}+\boldsymbol{\Pi}^\top \mathbf{C}^{-1}\right) \boldsymbol{\theta}
          $$
          By \cref{lemma: gaussian}, we obtain
          $$
            \boldsymbol{\theta}\mid \boldsymbol{q} \sim \mathcal{N}\left(\boldsymbol{\nu}, \boldsymbol{H}^{-1}\right), \quad \boldsymbol{\nu}=\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{P}+\boldsymbol{C}^{-1}\right]^{-1}\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{q}+\boldsymbol{C}^{-1} \boldsymbol{\Pi}\right] \text{ and } \boldsymbol{H}^{-1}=\left[\boldsymbol{P}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{P}+\boldsymbol{C}^{-1}\right]^{-1}
          $$
          as desired.

  \end{enumerate}
  \vspace*{0.8em}
  \hrule

  \textbf{APT Model:}

  Leveraging the powerful APT model (Roll \& Ross, 1980; Ross, 1976), the parameter vector $\boldsymbol{\theta}$ could be generalized to
  represent the means of unobservable latent factors in the APT
  model, which assumes a linear functional form:
  \begin{align}
    \boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon},
    \quad \boldsymbol{\epsilon}\sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}\right) \text{ and } \boldsymbol{D}=\operatorname{diag}\left(\sigma_1^2, \ldots, \sigma_n^2\right), \sigma_i^2>0,  \forall i
    \label{eq: APT model}
  \end{align}
  where $\boldsymbol{r}$ is an $n$-dimensional random vector containing the
  cross-section of returns in excess of the risk-free rate over some time interval
  $[t, t+1]$, and $\boldsymbol{X}$ is a (non-random) $n \times k$ matrix that is
  known before time $t$. The variable $\boldsymbol{f}$ in \cref{eq: APT model} denotes a
  $k$-dimensional latent random vector process, and
  information about the $\boldsymbol{f}$-process must be obtained via statistical
  inference. Specifically, we assume that the $\boldsymbol{f}$-process has finite
  first and second moments given by
  $$
    \mathbb{E}[\boldsymbol{f}]=\boldsymbol{\mu}_f \text { and } \mathbb{V}[\boldsymbol{f}]=\boldsymbol{F}
  $$
  In the Black-Litterman-Bayes model, we choose
  $$
    \boldsymbol{\theta} = \boldsymbol{\mu}_f,\quad
    \boldsymbol{f}\mid\boldsymbol{\theta} \sim \mathcal{N}\left(\boldsymbol{\theta}, \boldsymbol{F}\right)
  $$
  \textbf{Likelihood Function (Views):}

  For our understanding, let's assume we are considering two
  latent factors: value and momentum. Typically, a quantitative portfolio manager might have
  views on individual factors: (1) a view on the value premium, and (2) another
  view on the momentum premium. It would be atypical for portfolio managers to
  have views on a combination (e.g. linear) of factors. Hence to keep things simple but
  still useful, we take the views equation to be:
  $$
    \boldsymbol{\theta}=\boldsymbol{q}+\boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Omega}), \quad \boldsymbol{\Omega}=\operatorname{diag}\left(\omega_1^2, \ldots, \omega_k^2\right)
  $$
  then, $\boldsymbol{q} \mid \boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\theta},
    \boldsymbol{\Omega})$ and the corresponding likelihood function is therefore
  $$
    f(\boldsymbol{q} \mid \boldsymbol{\theta}) \propto \exp \left[-\frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{q})^{\top} \boldsymbol{\Omega}^{-1}(\boldsymbol{\theta}-\boldsymbol{q})\right]=\prod_{i=1}^k \exp \left[-\frac{1}{2 \omega_i^2}\left(\theta_i-q_i\right)^2\right]
  $$

  \textbf{Prior:}

  What prior for $\boldsymbol{\theta}$ should we choose?
  First, let's set the prior $\pi(\boldsymbol{\theta})$ to be
  $$
    \pi(\boldsymbol{\theta}) \sim \mathcal{N}(\boldsymbol{\xi}, \boldsymbol{V})
  $$
  where $\boldsymbol{\xi} \in \mathbb{R}^k$ and $\boldsymbol{V} \in S_{++}^k$, the
  set of symmetric positive definite $k \times k$ matrices.

  Choosing a prior then amounts to choosing $\boldsymbol{\xi}$ and $\boldsymbol{V}$, and once
  a prior is chosen, we instantly obtain the prior predictive distribution $p(\boldsymbol{r})$ for $\boldsymbol{r}$ using the APT model and
  an associated prior (benchmark) portfolio with holdings $\boldsymbol{h}_B$
  where $\boldsymbol{h}_B$ maximizes expected utility of wealth, where the expectation
  is taken with respect to the a priori distribution on asset returns
  $$
    p(\boldsymbol{r})=\int p(\boldsymbol{r} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d \boldsymbol{\theta},
    \quad \boldsymbol{h}_B = \arg \max_{\boldsymbol{h}} \EE\left[u\left(\boldsymbol{h}^\top\boldsymbol{r}\right)\right]
    = \arg \max_{\boldsymbol{h}} \int u\left(\boldsymbol{h}^\top \boldsymbol{r}\right) p(\boldsymbol{r}) d \boldsymbol{r}
  $$
  Let's see what our prior (benchmark) portfolio will look like.

  Since $\boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon},
    \boldsymbol{\epsilon}\sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}\right)$ and $\boldsymbol{f}\mid\boldsymbol{\theta} \sim \mathcal{N}\left(\boldsymbol{\theta}, \boldsymbol{F}\right)$,
  we can equivalently rewrite it as $\boldsymbol{f} = \boldsymbol{\theta} + \boldsymbol{w}$
  where $\boldsymbol{w} \sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{F}\right)$, and $\boldsymbol{w}$ is independent of $\boldsymbol{\theta}$.
  Then, by properties of multivariate normal distribution, we have
  $$
    \boldsymbol{r}=\boldsymbol{X} \boldsymbol{f}+\boldsymbol{\epsilon} =
    \boldsymbol{X} \boldsymbol{\theta}+\boldsymbol{X} \boldsymbol{w}+\boldsymbol{\epsilon}\quad
    \Longrightarrow \quad \boldsymbol{r}_{\pi} \sim \mathcal{N}\left(\boldsymbol{X}\boldsymbol{\xi}, \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
  $$
  The a priori optimal portfolio under CARA utility, $u(x) = -e^{-\lambda x}$, $\lambda \ne 0$, is then
  $$
    \boldsymbol{h}_B:=\boldsymbol{h}^*
    =\left(\lambda \mathbb{V}_\pi[\boldsymbol{r}]\right)^{-1} \mathbb{E}_\pi[\boldsymbol{r}]
    =\lambda^{-1} \left(\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)^{-1} \boldsymbol{X}\boldsymbol{\xi}
  $$

  \begin{remark}
    In the original paper, the author derived the prior predictive mean and variance as
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      \text{ and }
      \mathbb{V}_\pi[\boldsymbol{r}]=\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
    $$
    where $\boldsymbol{H}:=\boldsymbol{V}^{-1}+\boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{X}$ and $\boldsymbol{\Sigma}:=\boldsymbol{D}+\boldsymbol{X F} \boldsymbol{X}^\top$.

    We first note that due to his careless mistake, these results are wrong! The correct results are:
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      \quad \text{ and } \quad
      \mathbb{V}_\pi[\boldsymbol{r}]=\left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
    $$
    Then, substitute $\mathbf{A} = \boldsymbol{\Sigma}$, $\mathbf{U} = \mathbf{X}$, $\mathbf{V} = \mathbf{X}^\top$ and $\mathbf{B} = \mathbf{V}$ in the Woodbury identity (\cref{theorem: Woodbury}), we have
    $$
      \mathbb{V}_\pi[\boldsymbol{r}] = \left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}
      =\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{\Sigma} = \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}
    $$
    It follows that
    $$
      \left(\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1}\boldsymbol{\Sigma}^{-1} \boldsymbol{X}
      =\left(\boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{\Sigma}\right)\boldsymbol{\Sigma}^{-1} \boldsymbol{X}
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{X} + \boldsymbol{X}
      = \boldsymbol{X}\boldsymbol{V}\left(\boldsymbol{V}^{-1} + \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{X}\right)
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{H}
    $$
    The mean $\mathbb{E}_\pi[\boldsymbol{r}]$ then follows as
    $$
      \mathbb{E}_\pi[\boldsymbol{r}]
      =\left(\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{X}^\top \boldsymbol{\Sigma}^{-1}\right)^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi}
      = \boldsymbol{X}\boldsymbol{V}\boldsymbol{H}\boldsymbol{H}^{-1} \boldsymbol{V}^{-1} \boldsymbol{\xi} = \boldsymbol{X}\boldsymbol{\xi}
    $$
    Thus, we obtain
    $$
      \boldsymbol{r}_{\pi} \sim \mathcal{N}\left(\boldsymbol{X}\boldsymbol{\xi}, \boldsymbol{X}\boldsymbol{V}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
    $$
    which is consistent with our results.
  \end{remark}
  \begin{enumerate}[label=(\alph*), start=2]
    \item \textbf{Solution:} In this question, we derive the posterior distribution for
          $\boldsymbol{r}\mid \boldsymbol{q}$ and the mean-variance optimal
          portfolio holdings.

          \textbf{Step 1: Calculate the posterior for $\boldsymbol{\theta} \mid  \boldsymbol{q}$.}

          Since $\pi(\boldsymbol{\theta})$ is normal and the likelihood
          $f(\boldsymbol{q} \mid \boldsymbol{\theta})$ is also normal, the prior
          is a conjugate prior with respect to the normal likelihood. Then, the
          posterior distribution $p(\boldsymbol{\theta} \mid \boldsymbol{q})$ is
          also normal. Using this relation and $p(\boldsymbol{\theta} \mid \boldsymbol{q}) \propto f(\boldsymbol{q} \mid \boldsymbol{\theta})\pi(\boldsymbol{\theta})$, we have (neglecting terms that do not contain $\boldsymbol{\theta}$)
          $$
            -2 \log p(\boldsymbol{\theta} \mid \boldsymbol{q})= (\boldsymbol{\theta}-\boldsymbol{q})^{\top} \boldsymbol{\Omega}^{-1}(\boldsymbol{\theta}-\boldsymbol{q})+(\boldsymbol{\theta}-\boldsymbol{\xi})^{\top} \mathbf{V}^{-1}(\boldsymbol{\theta}-\boldsymbol{\xi})
            =  \boldsymbol{\theta}^{\top}\left(\boldsymbol{\Omega}^{-1}+\mathbf{V}^{-1}\right) \boldsymbol{\theta}-2\left(\boldsymbol{q}^{\top} \boldsymbol{\Omega}^{-1}+\boldsymbol{\xi}^{\top} \mathbf{V}^{-1}\right) \boldsymbol{\theta}
            +(\text { terms without } \boldsymbol{\theta})
          $$
          Setting
          $\mathbf{H}=\left(\boldsymbol{\Omega}^{-1}+\mathbf{V}^{-1}\right)$ and
          $\boldsymbol{\eta}^{\top}=\left(\boldsymbol{q}^{\top}
            \boldsymbol{\Omega}^{-1}+\boldsymbol{\xi}^{\top}
            \mathbf{V}^{-1}\right)$ in \cref{lemma: gaussian}, we have
          $$
            \mathbb{V}[\boldsymbol{\theta} \mid \boldsymbol{q}]=\mathbf{H}^{-1}=\left( \boldsymbol{\Omega}^{-1}+ \boldsymbol{V}^{-1}\right)^{-1},
            \quad \mathbb{E}[\boldsymbol{\theta} \mid  \boldsymbol{q}]=\mathbf{H}^{-1} \boldsymbol{\eta} =\mathbb{V}[\boldsymbol{\theta} \mid \boldsymbol{q}]\left(\boldsymbol{V}^{-1} \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} q\right)
            =\left( \boldsymbol{V}^{-1}+ \boldsymbol{\Omega}^{-1}\right)^{-1}\left( \boldsymbol{V}^{-1}  \boldsymbol{\xi}+ \boldsymbol{\Omega}^{-1}  \boldsymbol{q}\right)
          $$
          Therefore, the posterior must satisfy
          $$
            \boldsymbol{\theta} \mid  \boldsymbol{q} \sim \mathcal{N}\left(\widetilde{\boldsymbol{\xi}}, \widetilde{ \boldsymbol{V}}\right)
          $$
          where
          $\widetilde{\boldsymbol{V}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}$
          and $\widetilde{\boldsymbol{\xi}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}\left(\boldsymbol{V}^{-1}
            \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} \boldsymbol{q}\right)$.

          \textbf{Step 2: Calculate the posterior for $\boldsymbol{r} \mid  \boldsymbol{q}$.}

          The a posteriori distribution of asset returns $\boldsymbol{r}$ (also
          called the posterior predictive density) is given by
          $$
            p(\boldsymbol{r} \mid \boldsymbol{q})=\int p(\boldsymbol{r} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{q}) d \boldsymbol{\theta}
          $$
          For this one, we can view $p(\boldsymbol{\theta} \mid \boldsymbol{q})$
          as the prior density for $\boldsymbol{\theta}$, then the situation
          here is the same as in deriving the prior (benchmark) portfolio.
          Hence, use the previous result and make the substitution
          $\boldsymbol{\xi} \rightarrow \widetilde{\boldsymbol{\xi}}$ and
          $\boldsymbol{V} \rightarrow \widetilde{\boldsymbol{V}}$, we obtain
          $$
            \boldsymbol{r} \mid \boldsymbol{q} \sim \mathcal{N}\left(\boldsymbol{X}\widetilde{\boldsymbol{\xi}}, \boldsymbol{X}\boldsymbol{\widetilde{V}}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)
          $$

          \textbf{Step 3: Calculate the mean-variance optimal portfolio.}

          The posterior optimal portfolio under CARA utility is then
          $$
            \boldsymbol{h}^*
            =\left(\lambda \mathbb{V}[\boldsymbol{r}\mid \boldsymbol{q}]\right)^{-1} \mathbb{E}[\boldsymbol{r}\mid \boldsymbol{q}]
            =\lambda^{-1} \left(\boldsymbol{X}\boldsymbol{\widetilde{V}}\boldsymbol{X}^\top + \boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top + \boldsymbol{D}\right)^{-1} \boldsymbol{X}\boldsymbol{\widetilde{\xi}}
          $$
          where
          $\widetilde{\boldsymbol{V}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}$
          and $\widetilde{\boldsymbol{\xi}}=\left(\boldsymbol{V}^{-1}+\boldsymbol{\Omega}^{-1}\right)^{-1}\left(\boldsymbol{V}^{-1}
            \boldsymbol{\xi}+\boldsymbol{\Omega}^{-1} \boldsymbol{q}\right)$.
  \end{enumerate}
\end{topic}



\section{Topic: Important Properties of the Kalman Filter}
\begin{topic}
  \textbf{State Space Models:}

  Consider the following discrete linear dynamical system, which also referred to as linear state space model:
  \begin{align}
    \begin{aligned}
       & \mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t} \\
       & \mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}
    \end{aligned}\quad \text{ where } \quad
    \mathbf{x}_{0} \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right),\quad
    \mathbf{w}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{Q}_{t}\right), \quad
    \mathbf{v}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{R}_{t}\right)
    \label{eq: state space model relation}
  \end{align}
  Additionally, the initial state $\mathbf{x}_{0}$ and the noise terms $\mathbf{w}_{t},
    \mathbf{v}_{t}$ are all assumed to be mutually independent.

  Sequences $\{\mathbf{x}_{\tau}\}_{\tau=0}^t$ and $\{\mathbf{y}_{\tau}\}_{\tau=0}^t$ are
  called the hidden/latent state and observation sequences, respectively.
  It follows that the first and second equations are referred to the state and observation
  equations, respectively.
  In a state space model the state sequences $\{\mathbf{x}_{\tau}\}_{\tau=0}^t$ are not observable;
  however, the observation sequences $\{\mathbf{y}_{\tau}\}_{\tau=0}^t$ are fully observable.


  \textbf{The Filtering Problem:}

  The filtering problem is to estimate
  $$
    \hat{\mathbf{x}}_{t}=\mathbb{E}\left(\mathbf{x}_{t} \mid \mathbf{Y}_{t}\right)
  $$
  where $\mathbf{Y}_{t}:=\left\{\mathbf{y}_{0}, \ldots, \mathbf{y}_{t}\right\}$.
  In other words, given the noisy observations,
  $\left\{\mathbf{y}_{\tau}\right\}_{\tau=0}^{t}$, we seek to estimate the
  expected state. And the optimal solution for our setup is given by the Kalman filter, also referred
  to as linear quadratic estimation (LQE).
  \vspace{0.8em}

  \hrule

  \textbf{Derive the Kalman Filter:}

  The derivation of the Kalman filter is by induction, showing that the
  observation and state sequences are conditionally Gaussian.

  \textbf{Base Case:} Since $\mathbf{x}_{0}$ is Gaussian, $\mathbf{x}_{1}, \mathbf{y}_{1}$ are also Gaussians.
  It follows that
  $$
    \begin{aligned}
       & \mathbf{x}_{1} \sim \mathcal{N}\left(\mathbf{F}_{1} \boldsymbol{\mu}_{0}, \mathbf{F}_{1} \mathbf{P}_{0} \mathbf{F}_{1}^\top+\mathbf{Q}_{1}\right)                                                                              \\
       & \mathbf{y}_{1} \sim \mathcal{N}\left(\mathbf{H}_{1} \mathbf{F}_{1} \boldsymbol{\mu}_{0}, \mathbf{H}_{1}\left(\mathbf{F}_{1} \mathbf{P}_{0} \mathbf{F}_{1}^\top+\mathbf{Q}_{1}\right) \mathbf{H}_{1}^\top+\mathbf{R}_{1}\right)
    \end{aligned}
  $$

  \textbf{Induction Step:} To perform the induction step, we assume that
  $$
    \mathbf{x}_{t-1} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{P}_{t-1 \mid t-1}\right)
  $$

  where the subscript $t \mid t^{\prime}$ to denote our belief about the state $\mathbf{x}_{t}$ given
  observations $\mathbf{Y}_{t^{\prime}}$. We immediately obtain from \cref{eq: state space model relation}

  $$
    \begin{aligned}
       & \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right)                                                                              \\
       & \mathbf{y}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{H}_{t} \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top+\mathbf{R}_{t}\right)
    \end{aligned}
  $$

  As $\mathbf{x}_{t}$, $\mathbf{w}_{t}$ and $\mathbf{v}_{t}$ are independent, the covariance of
  $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$ is given by
  $$
    \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{y}_{t}\right)
    =\operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}\right)
    =\operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{H}_{t} \mathbf{x}_{t}\right)
    + \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{v}_{t}\right)
    = \EE\left[\mathbf{x}_{t}\mathbf{x}_{t}^\top\mathbf{H}_{t}^\top\right]
    =\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
  $$

  \textbf{IS/Predict Step of the Kalman Filter:} From the distribution of $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, we introduce the notation
  $$
    \begin{aligned}
      \hat{\mathbf{x}}_{t \mid t-1} & :=\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}                              \\
      \mathbf{P}_{t \mid t-1}       & :=\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}
    \end{aligned}
  $$
  Then, we see that the joint distribution of
  $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$, conditioned on $\mathbf{Y}_{t-1}$, is given by the multivariate Gaussian
  $$
    \left[\begin{array}{l}
        \mathbf{x}_{t} \\
        \mathbf{y}_{t}
      \end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
        \hat{\mathbf{x}}_{t \mid t-1} \\
        \mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}
      \end{array}\right],\left[\begin{array}{cc}
        \mathbf{P}_{t \mid t-1}                & \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top                               \\
        \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} & \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t}
      \end{array}\right]\right)
  $$
  \textbf{IS/Update Step of the Kalman Filter:}
  Let us now assume $\mathbf{y}_{t}$ is observed.
  Applying the conditional formulas for the multivariate Gaussian we obtain
  $$
    \mathbf{x}_{t} \mid \mathbf{Y}_{t} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t \mid t}, \mathbf{P}_{t \mid t}\right)
  $$
  where
  \begin{align*}
    \hat{\mathbf{x}}_{t \mid t} :=\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right), \quad
    \mathbf{P}_{t \mid t} :=\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1},\quad
    \mathbf{K}_{t} :=\mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top \mathbf{S}_{t}^{-1}, \quad
    \mathbf{S}_{t} :=\mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t}
  \end{align*}
  Specifically, $\mathbf{K}_{t}$ is called the Kalman gain. Clearly, our belief about
  $\mathbf{x}_{t}$ given $\mathbf{Y}_{t}$ is a multivariate Gaussian distribution.

  \begin{remark}
    From the above filter formulas, it follows that we can apply the predict and
    update steps recursively as new observations arrive. This makes the Kalman
    filter ideal for online real-time processing of information.
  \end{remark}
\end{topic}

\begin{problem}
Choose three out of the four subproblems. It is optional to solve the other one
for extra credit.
\begin{enumerate}[label=(\alph*)]
  \item In class we derived the Kalman filter under the assumption that
        $\mathbb{E}\left[\mathbf{w}_t \mathbf{v}_t^\top\right]=\mathbf{0}$. Now,
        derive the Kalman filter under the assumption that
        $\mathbb{E}\left[\mathbf{w}_t \mathbf{v}_t^\top\right]=\mathbf{M}_t$.

        \begin{solution}
          We assume that
          $$
            \mathbf{x}_{t-1} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{P}_{t-1 \mid t-1}\right)
          $$
          where the subscript $t \mid t^{\prime}$ to denote our belief about the state $\mathbf{x}_{t}$ given
          observations $\mathbf{Y}_{t^{\prime}}$.

          \textbf{Find $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$ and $\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}$:}

          From \cref{eq: state space model relation}, we get $\mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t}$.
          Then, for $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, it's easy to deduce that
          $$
            \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right)
          $$

          Since $\mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t} =
            \mathbf{H}_{t} \mathbf{F}_{t}
            \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}$, we have
          $\EE\left[\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}\right] = \mathbf{H}_{t}
            \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}$. By \cref{lemma: covariance matrix properties}, we obtain
          \begin{align*}
            \VV\left[\mathbf{y}_{t} \mid \mathbf{Y}_{t-1}\right]
            = \VV\left[\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right]
             & =  \operatorname*{Cov}\left(\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right) \\
            \mathbf{x}_{t-1} \text{ independent of } \mathbf{w}_{t} \text{ and } \mathbf{v}_{t} \Longrightarrow
             & =  \operatorname*{Cov}\left(\mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}\right)
            + \operatorname*{Cov}\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}, \mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)                                                                                                   \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \EE\left[\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)\left(\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)^\top\right]                                                                                      \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \mathbf{H}_{t}\EE\left[\mathbf{w}_{t}\mathbf{w}_{t}^\top\right]\mathbf{H}_{t}^\top
            + \EE\left[\mathbf{v}_{t}\mathbf{v}_{t}^\top\right]
            + \EE\left[\mathbf{H}_{t}\mathbf{w}_{t}\mathbf{v}_{t}^\top\right]
            + \EE\left[\mathbf{v}_{t}\mathbf{w}_{t}^\top\mathbf{H}_{t}^\top\right]                                                                                                                                                       \\
             & = \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1}\mathbf{F}_{t}^\top \mathbf{H}_{t}^\top
            + \mathbf{H}_{t}\mathbf{Q}_{t}\mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top                                                                                                                                                                                     \\
             & =\mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
          \end{align*}
          Therefore, we have
          \begin{align*}
            \mathbf{x}_{t} \mid \mathbf{Y}_{t-1} & \sim \mathcal{N}\left(\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \\
            \mathbf{y}_{t} \mid \mathbf{Y}_{t-1}
                                                 & \sim
            \mathcal{N}\left(\mathbf{H}_{t} \mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1},
            \mathbf{H}_{t}\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top
            + \mathbf{R}_{t}
            + \mathbf{H}_{t}\mathbf{M}_{t}
            + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top\right)
          \end{align*}

          \textbf{Find $\mathbf{x}_{t} \mid \mathbf{Y}_{t}$:}

          As $\mathbf{x}_{t-1}$, $\mathbf{w}_{t}$ and $\mathbf{x}_{t-1}$, $\mathbf{v}_{t}$ are independent, the covariance of
          $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$ is given by
          $$
            \operatorname{Cov}\left(\mathbf{x}_{t}, \mathbf{y}_{t}\right)
            =\operatorname{Cov}\left(\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)
            =\operatorname{Cov}\left(\mathbf{F}_{t} \mathbf{x}_{t-1}, \mathbf{H}_{t} \mathbf{F}_{t} \mathbf{x}_{t-1}\right)
            +\operatorname{Cov}\left(\mathbf{w}_{t}, \mathbf{H}_{t}\mathbf{w}_{t}+\mathbf{v}_{t}\right)
            =\left(\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}\right) \mathbf{H}_{t}^\top + \mathbf{M}_{t}
          $$

          \textbf{- IS/Predict Step of the Kalman Filter:} From the distribution of $\mathbf{x}_{t} \mid \mathbf{Y}_{t-1}$, we introduce the notation
          $$
            \begin{aligned}
              \hat{\mathbf{x}}_{t \mid t-1} & :=\mathbf{F}_{t} \hat{\mathbf{x}}_{t-1 \mid t-1}                              \\
              \mathbf{P}_{t \mid t-1}       & :=\mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^\top+\mathbf{Q}_{t}
            \end{aligned}
          $$
          Then, we see that the joint distribution of
          $\mathbf{x}_{t}$ and $\mathbf{y}_{t}$, conditioned on $\mathbf{Y}_{t-1}$, is given by the multivariate Gaussian
          $$
            \left[\begin{array}{l}
                \mathbf{x}_{t} \\
                \mathbf{y}_{t}
              \end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
                \hat{\mathbf{x}}_{t \mid t-1} \\
                \mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}
              \end{array}\right],\left[\begin{array}{cc}
                \mathbf{P}_{t \mid t-1}                                       & \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top  + \mathbf{M}_{t}                                                                                     \\
                \mathbf{H}_{t} \mathbf{P}_{t \mid t-1}  + \mathbf{M}_{t}^\top & \mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t} + \mathbf{H}_{t}\mathbf{M}_{t} + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
              \end{array}\right]\right)
          $$
          \textbf{- IS/Update Step of the Kalman Filter:}
          Let us now assume $\mathbf{y}_{t}$ is observed.
          Applying the conditional formulas for the multivariate Gaussian we obtain
          $$
            \mathbf{x}_{t} \mid \mathbf{Y}_{t} \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t \mid t}, \mathbf{P}_{t \mid t}\right)
          $$
          where
          \begin{align*}
            \hat{\mathbf{x}}_{t \mid t} :=\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right), ~~
            \mathbf{P}_{t \mid t} :=\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1} - \mathbf{K}_{t}\mathbf{M}_{t}^\top, ~~
            \mathbf{K}_{t} :=\mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top  + \mathbf{M}_{t}, ~
            \mathbf{S}_{t} :=\mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^\top+\mathbf{R}_{t} + \mathbf{H}_{t}\mathbf{M}_{t} + \mathbf{M}_{t}^\top\mathbf{H}_{t}^\top
          \end{align*}
          Specifically, $\mathbf{K}_{t}$ is called the Kalman gain. Clearly, our belief about
          $\mathbf{x}_{t}$ given $\mathbf{Y}_{t}$ is a multivariate Gaussian distribution.
        \end{solution}

  \item Using the notation from class, let us denote the error between the true
        and estimated states by the random variable
        $\widetilde{\mathbf{x}}_t=\mathbf{x}_t-\hat{\mathbf{x}}_t$. We denote by
        $\mathbf{W}_t$ a known positive definite matrix. Show that the Kalman filter
        is the solution to the problem
        \begin{align}
          \min_{\hat{\mathbf{x}}_t} \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]\label{eq: kalman}
        \end{align}

        \begin{solution}
          Since $\mathbf{W}_t$ is a known positive definite matrix, by Cholesky
          decomposition, we have $\mathbf{W}_t =
            \mathbf{L}_t\mathbf{L}_t^\top$. Recall that the Kalman filter approximates
          $\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]$ as $\hat{\mathbf{x}}_t = \hat{\mathbf{x}}_{t\mid t}$,
          and using the fact that $\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t
            \widetilde{\mathbf{x}}_t\in \mathbb{R}$, we can rewrite the loss
          function as
          \begin{align*}
            \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]
             & = \mathbb{E}\left[\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)^\top \mathbf{W}_t \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]
            + \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\right]                  \\
             & \quad + 2\cdot \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]       \\
             & =  \mathbb{E}\left[\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)^\top \mathbf{L}_t\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]
            + \mathbb{E}\left[\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{L}_t\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\right] \\
            \phi(\hat{\mathbf{x}}_t) =\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)^\top \mathbf{W}_t \Longrightarrow
             & \quad + 2\mathbb{E}\left\{\mathbb{E}\left[\phi(\hat{\mathbf{x}}_t)\left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\mid \mathbf{Y}_t\right]\right\}                                             \\
             & =  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
            + \mathbb{E}\left[\|\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\|^2\right]
            + 2\mathbb{E}\left\{\mathbb{E}\left[\phi(\hat{\mathbf{x}}_t)\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\right]\right\}                                      \\
             & =  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
            + \mathbb{E}\left[\|\mathbf{L}_t^\top\left(\EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right] - \hat{\mathbf{x}}_t\right)\|^2\right]                                                                                            \\
             & \ge  \mathbb{E}\left[\|\mathbf{L}_t^\top \left(\mathbf{x}_t - \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]\right)\|^2\right]
          \end{align*}
          where the equality can be attained when $\hat{\mathbf{x}}_t = \EE\left[\mathbf{x}_t\mid \mathbf{Y}_t\right]$.

          Therefore, the Kalman filter is the solution to the problem $\min_{\hat{\mathbf{x}}_t} \mathbb{E}\left[\widetilde{\mathbf{x}}_t^\top \mathbf{W}_t \widetilde{\mathbf{x}}_t\right]$.
        \end{solution}
  \item \textbf{(The Kalman filter is the Best LINEAR Predictor under Mean-Squared Error if noise is NOT Gaussian (Non-linear Predictors could be better.))} Now
        let us assume that $\mathbf{w}_t$ and $\mathbf{v}_t$ have zero mean, are
        uncorrelated with covariance matrices $\mathbf{Q}_t$ and $\mathbf{R}_t$,
        respectively (but they are no longer Gaussian). Show that the Kalman filter is
        the best linear solution to \cref{eq: kalman}.

        \begin{solution}
          We assume that the estimate is a
          linear weighted sum of the prediction and the new observation and can
          be described by the equation,
          $$
          \hat{\mathbf{x}}_{t \mid t} :=\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right)
          $$
          where $\mathbf{K}_{t}$ is called the
          gain matrice. Our problem now is to reduced to
          finding the $\mathbf{K}_{t}$ that
          minimise the conditional mean squared estimation error where of course
          the estimation error is given by:
          $$
            \widetilde{\mathbf{x}}_{t \mid t} = \mathbf{x}_{t} -\hat{\mathbf{x}}_{t \mid t}
          $$
          \textbf{Unbiasedness:}
          Assume that at state $t - 1$, our predictor $E\left[\hat{\mathbf{x}}_{t-1 \mid t-1}\right]=E\left[\mathbf{x}_{t-1}\right]$ is unbiased. Notice that
          $$
              E\left[\hat{\mathbf{x}}_{t \mid t-1}\right] =E\left[\mathbf{F}_{t-1} \hat{\mathbf{x}}_{t-1 \mid t-1} + \mathbf{w}_{t-1}\right] =\mathbf{F}_{t-1} E\left[\hat{\mathbf{x}}_{t-1 \mid t-1}\right]
              =E\left[\mathbf{x}_{t}\right]
          $$
          Taking the expectation yields
          $$
            \begin{aligned}
              E\left[\hat{\mathbf{x}}_{t \mid t}\right] =E\left[\hat{\mathbf{x}}_{t \mid t-1}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right)\right] 
               =E\left[\mathbf{x}_{t}\right] + \EE\left[\mathbf{K}_{t}\mathbf{H}_t\mathbf{x}_{t} + \mathbf{K}_{t}\mathbf{v}_{t} -\mathbf{K}_{t}\mathbf{H}_{t} \hat{\mathbf{x}}_{t \mid t-1}\right] = E\left[\mathbf{x}_{t}\right]
            \end{aligned}
          $$
          Therefore, the result given by the Kalman filter formulated as above is unbiased provided $E\left[\hat{\mathbf{x}}_{t \mid t}\right]=E\left[\mathbf{x}_{k}\right]$.

            \textbf{Formulate the Error Covariance:}

          We now turn to the updated error covariance
          \begin{align*}
              E\left[\widetilde{\mathbf{x}}_{t \mid t} \widetilde{\mathbf{x}}_{t \mid t}^\top \mid \mathbf{Y}_k\right]                                                                                                           
              =E\left[\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t \mid t}\right)\left(\mathbf{x}_{k}-\hat{\mathbf{x}}_{t \mid t}\right)^\top\right]                                                                                    
              =\mathbf{P}_{t \mid t} =\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1}
          \end{align*}
          Our goal is now to minimise the conditional mean-squared estimation
          error with respect to the Kalman gain, $\mathbf{K}$.
          $$
              L =\min _{\mathbf{K}_{t}} E\left[\widetilde{\mathbf{x}}_{t \mid t}^\top \widetilde{\mathbf{x}}_{t \mid t} \mid \mathbf{Y}_{t}\right]                                  
                =\min _{\mathbf{K}_{k}} \operatorname{trace}\left(E\left[\widetilde{\mathbf{x}}_{t \mid t} \widetilde{\mathbf{x}}_{t \mid t}^\top \mid \mathbf{Y}_{t}\right]\right) 
                =\min _{\mathbf{K}_{k}} \operatorname{trace}\left(\mathbf{P}_{t \mid t}\right)
          $$

          For any matrix $\mathbf{A}$ and a symmetric matrix $\mathbf{B}$
          $$
            \frac{\partial}{\partial \mathbf{A}}\left(\operatorname{trace}\left(\mathbf{A B A}^T\right)\right)=2 \mathbf{A B}
          $$
          (to see this, consider writing the trace as $\sum_i \mathbf{a}_i^T
          \mathbf{B} \mathbf{a}_i$ where $\mathbf{a}_i$ are the columns of
          $\mathbf{A}^T$, and then differentiating w.r.t. the $\mathbf{a}_i$ ).

          Differentiating with respect to
          the gain matrix (using the relation above) and setting equal to zero
          yields
          $$
            \frac{\partial L}{\partial \mathbf{K}_{t}}=-2\left(\mathbf{I}-\mathbf{K}_{t} \mathbf{H}_{t}\right) \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^T+2 \mathbf{K}_{t} \mathbf{R}_{t}=\mathbf{0}
          $$

          Re-arranging gives an equation for the gain matrix
          $$
            \mathbf{K}_{t}=\mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^T\left[\mathbf{H}_{t} \mathbf{P}_{t \mid t-1} \mathbf{H}_{t}^T+\mathbf{R}_{t}\right]^{-1}
          $$
          This is exactly what we have. Thus, the result given by the Kalman filter formulated as above is the best unbiased linear predictor provided
          $E\left[\hat{\mathbf{x}}_{t \mid t}\right]=E\left[\mathbf{x}_{k}\right]$.

        \end{solution}
  \item In class we derived the Kalman filter under the assumption that
        $\mathbf{w}_t$ and $\mathbf{v}_t$ are uncolored (i.e. each is serially
        uncorrelated).
        \begin{enumerate}[label=(\roman*)]
          \item Derive the Kalman filter under the assumption that
                $\mathbf{w}_t$ is a VAR(1) process with known system matrix.
          \item Derive the Kalman filter under the assumption that
                $\mathbf{v}_t$ is a $\operatorname{VAR}(1)$ process with known
                system matrix.
        \end{enumerate}
        Hint: For (d), both (i) and (ii) can be solved by properly augmenting
        the state equations. Can you find a solution to (ii) where one does not
        have to augment the state? Is it possible to do so for (i) - why, or why
        not?

        \begin{solution}

        \end{solution}
\end{enumerate}

\end{problem}


\section{Topic: Kalman filter's Application on Financial Problems --- Pairs Trading}
\begin{problem}
In this question we consider a basic pairs trading strategy between two stocks
with prices $p_t^A$ and $p_t^B$ at time $t$. We denote the spread between them
by $s_t:=\log \left(p_t^A\right)-\log \left(p_t^B\right)$ and assume the spread
follows an Ornstein-Uhlenbeck process
\begin{align}
  d s_t=\kappa\left(\theta-s_t\right) d t+\sigma d B_t \label{eq: OU process}
\end{align}
where $d B_t$ is a standard Brownian motion. In other words, the spread reverts
to its mean $\theta \in \mathbb{R}$ at the speed $\kappa \in \mathbb{R}_{+}$and
volatility $\sigma \in \mathbb{R}_{+}$.
\begin{enumerate}[label=(\alph*)]
  \item In the discrete sense, we use the notation $S_n := S_{n\cdot \Delta t}$
        to denote the value of process $s$ at a given time $n\cdot \Delta t$ where
        $\Delta t$ is a constant period amount of time picked as the step size.
        Show that the discrete time solution of \cref{eq: OU process} is
        Markovian, that is
        $$
          s_n=\mathbb{E}\left[s_n \mid s_{n-1}\right]+\varepsilon_n
        $$
        where $n=1,2, \ldots$, and $\varepsilon_n$ is a random process with zero
        mean and variance equal to $\sigma_{\varepsilon,
            n}^2=\mathbb{V}\left[s_n \mid s_{n-1}\right]$.

        Hint: You can derive the discrete solution explicitly.

        \begin{solution}
          Set $f(t, x) = e^{\kappa t}x$ and apply the It\^{o}'s lemma, we have
          $de^{e^{\kappa t}s_t} = \kappa e^{\kappa t}s_t dt + e^{\kappa t} ds_t$. Pluging in \cref{eq: OU process}, we obtain
          $$
            d s_t + \kappa s_t d t = \frac{d e^{\kappa t}s_t}{e^{\kappa t}} = \kappa \theta dt+\sigma d B_t
            \Longrightarrow
            e^{\kappa t}s_t - s_0 = \int_{0}^t e^{\kappa s}\kappa \theta ds + \int_{0}^t e^{\kappa s}\sigma d B_s
            \Longrightarrow
            s_t = e^{-\kappa t}s_0 + \theta \left(1 - e^{-\kappa t}\right) + \int_{0}^t e^{\kappa(s-t)}\sigma d B_s, \forall t>0
          $$
          To extract $s_{n-1}$ out of $s_n$, we can rewrite $s_n$ as:
          \begin{align*}
            s_n
             & = e^{-\kappa n\Delta t}s_0 + \theta \left(1 - e^{-\kappa n\Delta t}\right) + \int_{0}^n e^{\kappa(s-n)\Delta t}\sigma d B_s                                                                                                                                                                                           \\
             & = \theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} \cdot \left(e^{-\kappa (n-1)\Delta t}s_0 + \theta \left(1 - e^{-\kappa (n-1)\Delta t}\right) + e^{-\kappa (n-1)\Delta t} \left[\int_{0}^{(n-1)\Delta t} e^{\kappa s}\sigma d B_s + \int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s\right]\right) \\
             & = \theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} s_{n-1} + e^{-\kappa n\Delta t}\int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s
          \end{align*}
          Taking the mean and variance of $s_{n}$ conditioned on $s_{n-1}$, and define $\varepsilon_n := e^{-\kappa n\Delta t}\int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s$, we have
          \begin{align*}
            \mathbb{E}\left[s_n \mid s_{n-1}\right]
             & = \mathbb{E}\left[\theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} s_{n-1}\right] + \mathbb{E}\left[e^{-\kappa n\Delta t}\int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s \mid s_{n-1}\right]
            \xlongequal[\text{ information up to } n-1 ]{ s_{n-1} \text{ only contains }} \theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} s_{n-1}                                                                    \\
            \mathbb{V}\left[s_n \mid s_{n-1}\right]
             & = \mathbb{V}\left[\theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} s_{n-1} + e^{-\kappa n\Delta t}\int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s \mid s_{n-1}\right]
            = \mathbb{V}\left[e^{-\kappa n\Delta t}\int_{(n-1)\Delta t}^{n\Delta t} e^{\kappa s}\sigma d B_s \right]
            = \frac{\sigma^2}{2\kappa} \left(1-e^{-2\kappa \Delta t}\right) = \mathbb{V}\left[\varepsilon_n \right]
          \end{align*}
          Therefore, we have
          $$
            s_n = \mathbb{E}\left[s_n \mid s_{n-1}\right] + \varepsilon_n
          $$
          where $\varepsilon_n$ is a random process with zero
          mean and variance equal to $\sigma_{\varepsilon,
              n}^2=\mathbb{V}\left[s_n \mid s_{n-1}\right] = \frac{\sigma^2}{2\kappa} \left(1-e^{-2\kappa \Delta t}\right)$.

        \end{solution}
  \item Propose a methodology for updating the parameters $\theta, \kappa$ using
        the Kalman filter and describe how you would use it to trade the stock pair.

        \begin{solution}
          To conform the notation used in the Kalman filter, we use $t\in
            \mathbb{N}$ to denote the states and write the result in (a) as:
          $$
            s_t = \theta - e^{-\kappa\Delta t}\theta + e^{-\kappa\Delta t} s_{t-1} + \varepsilon_t \quad  \text{ where } \quad  \varepsilon_t \sim \mathcal{N}\left(0, \frac{\sigma^2}{2\kappa} \left(1-e^{-2\kappa \Delta t}\right)\right)
          $$
          \textbf{State Equation:}

          Set constants $\alpha = \theta - e^{-\kappa\Delta t}\theta$, $\beta =
            e^{-\kappa\Delta t}$, $\mathbf{F}_t = \begin{bmatrix} 1      & 0     \\
                \alpha & \beta
            \end{bmatrix} $, $\mathbf{x}_t = \begin{bmatrix}
              1 \\
              s_t
            \end{bmatrix}$, $\mathbf{w}_t = \begin{bmatrix}
              0 \\
              \varepsilon_t
            \end{bmatrix} $
          and
          $\mathbf{Q}_{t} = \begin{bmatrix}
              0 & 0                      \\
              0 & \sigma_{\epsilon, t}^2
            \end{bmatrix}$, we have our state equation as:
          $$
            s_t = \alpha + \beta s_{t-1} + \varepsilon_t
            \quad \xrightarrow{ \text{ rewrite } } \quad
            \begin{bmatrix}
              1 \\
              s_t
            \end{bmatrix}
            =
            \begin{bmatrix}
              1      & 0     \\
              \alpha & \beta
            \end{bmatrix}
            \begin{bmatrix}
              1 \\
              s_{t-1}
            \end{bmatrix}
            + \begin{bmatrix}
              0 \\
              \varepsilon_t
            \end{bmatrix}
            \quad \xrightarrow{ \text{ substitute } \mathbf{F}_t, \mathbf{x}_t \text{ and } \mathbf{w}_t} \quad
            \mathbf{x}_t = \mathbf{F}_t\mathbf{x}_{t-1} + \mathbf{w}_t
            \quad  \text{ where } \quad
            \mathbf{w}_t \sim \mathcal{N}\left(\boldsymbol{0},  \mathbf{Q}_{t}\right)
          $$
          \textbf{Observations Equation:}

          Now, assume the observation $y_t$ is just the true process $s_t$ plus a small perturbation.
          Setting $\mathbf{H}_t = \begin{bmatrix}
              0 & 1
            \end{bmatrix}$, $\mathbf{v}_t = v \sim \mathcal{N}\left(0, \nu^2\right)$ and
          $\mathbf{R}_{t} = \nu^2$, we obtain
          $$
            y_t = \mathbf{H}_t\mathbf{x}_t + \mathbf{v}_t
            \quad  \text{ where } \quad
            \mathbf{v}_t \sim \mathcal{N}\left(\boldsymbol{0},  \mathbf{R}_{t}\right)
          $$
          \textbf{Initial State:}

          Naturally, we set the initial state variable $\mathbf{x}_0  \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right)$
          where $\boldsymbol{\mu}_{0} = \begin{bmatrix}
              1 \\
              s_0
            \end{bmatrix}$ and $\mathbf{P}_{0} = \boldsymbol{0}$.

          Therefore, by our construction, we have the linear state space model:
          \begin{align*}
            \begin{aligned}
               & \mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t} \\
               & \mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}
            \end{aligned}\quad \text{ where } \quad
            \mathbf{x}_{0} \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right),\quad
            \mathbf{w}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{Q}_{t}\right), \quad
            \mathbf{v}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{R}_{t}\right)
          \end{align*}
          Additionally, the initial state $\mathbf{x}_{0}$ and the noise terms $\mathbf{w}_{t},
            \mathbf{v}_{t}$ are all assumed to be mutually independent.
        \end{solution}
  \item Test your methodology from (b) on simulated data. In particular, (i)
        simulate (2) from known parameters $\theta, \kappa$ and $\sigma$, and then
        (ii) use the Kalman filter to recover them. You do not need to implement the
        Kalman filter from scratch; you are welcome to use a Kalman implementation
        from a Python package such as pykalman. How do you obtain a good estimate of
        $\sigma$ ?

        \begin{solution}

        \end{solution}
  \item Repeat the same experiment from (c), but this time simulate (2) first
        with $\kappa$ having the same value as above and then suddenly changing it to
        another value such that the half-life of the spread is $50 \%$ of its original
        value. How long does it take the Kalman filter to adjust? Can you make
        adjustment to your filter in order to speed up the time it takes the Kalman
        filter to adjust?

        \begin{solution}

        \end{solution}
\end{enumerate}
Hint: For (c) and (d), think about how you are going to demonstrate the results
using appropriate graphs, etc.
\end{problem}

\section{Topic: Kalman filter's Application on Financial Problems --- Index Tracking Portfolios}

\begin{problem}
It is common in portfolio management to build so-called (index) tracking
portfolios. Let us assume we are observing the return of the S\&P 500 benchmark
index, $r_{b, t}$. Now, let us pick a subset of 50 stocks from the constituents
of this index. We will use these stocks to build a tracking portfolio for the
index. For example, this could be the 50 companies in the index with the largest
market cap. We denote the returns of these 50 stock by $\mathbf{r}_t \in
  \mathbb{R}^{50}$. The goal of finding a tracking portfolio is to find a dynamic
trading strategy of the 50 stocks such that $\boldsymbol{\beta}_t^\top
  \mathbf{r}_t \approx r_{b, t}$, where $\boldsymbol{\beta}_t$ denotes the
holdings (relative weights) of the tracking portfolio.
\begin{enumerate}[label=(\alph*)]
  \item In this part, we assume that the covariance matrix of returns of the
        stocks in the $\mathrm{S} \& \mathrm{P} 500, \boldsymbol{\Sigma}$, is given
        and constant through time. Find the portfolio of these 50 stocks that
        minimizes the tracking error to $r_{b, t}$, i.e. find the solution to
        $$
          \boldsymbol{\beta}_t^*=\operatorname{argmin}_{\boldsymbol{\beta}_t} \sqrt{\mathbb{V}\left[r_{b, t}-\boldsymbol{\beta}_t^\top \mathbf{r}_t\right]} .
        $$
        What specific property does $\boldsymbol{\beta}_t^*$ have here?

        \begin{solution}

          \textbf{Formulation 1:}

          Suppose $\boldsymbol{S}$ and $\boldsymbol{\mu}_t =
            \EE\left[\boldsymbol{r}_t\right]$ are the covariance matrix and the
          mean of the returns of the $50$ stocks we picked, respectively.
          Additionally, let $\boldsymbol{e} = (1,\ldots, 1)\in \mathbb{R}^{50}$ denote the vector of all $1$'s. Since
          $r_{b, t}$ is deterministic, $\mathbb{V}\left[r_{b,
                  t}-\boldsymbol{\beta}_t^\top \mathbf{r}_t\right] =
            \mathbb{V}\left[\boldsymbol{\beta}_t^\top \mathbf{r}_t\right] =
            \EE\left[\boldsymbol{\beta}_t^\top \mathbf{r}_t
              \mathbf{r}_t^\top\boldsymbol{\beta}_t\right] =
            \boldsymbol{\beta}_t^\top \boldsymbol{S}\boldsymbol{\beta}_t$
          and $\boldsymbol{\beta}_t^\top\mathbf{r}_t \approx r_{b, t}$, we have
          $$
            \boldsymbol{\beta}_t^*
            =\operatorname{argmin}_{\boldsymbol{\beta}_t} \sqrt{\boldsymbol{\beta}_t^\top \boldsymbol{S}\boldsymbol{\beta}_t}
            =\operatorname{argmin}_{\boldsymbol{\beta}_t} \frac{1}{2}\boldsymbol{\beta}_t^\top \boldsymbol{S}\boldsymbol{\beta}_t
            \quad \text{ given } \quad \boldsymbol{\beta}_t^\top
            \EE\left[\mathbf{r}_t\right] = \boldsymbol{\beta}_t^\top
            \boldsymbol{\mu}_t= r_{b, t}, \quad \boldsymbol{\beta}_t^\top\boldsymbol{e} = 1
          $$
          Applying the Lagrange multipliers, we have Lagrangian
          $$
            L \equiv L(\boldsymbol{\beta}_t, \lambda, \gamma):=\frac{1}{2} \boldsymbol{\beta}_t^{\top} \boldsymbol{S} \boldsymbol{\beta}_t+\lambda\left(1-\boldsymbol{\beta}_t^{\top} \boldsymbol{e}\right)+\gamma\left(r_{b, t}-\boldsymbol{\beta}_t^{\top} \boldsymbol{\mu}_t\right)
          $$
          The first-order necessary condition then yields
          \begin{align}
            F O C : \frac{\partial L}{\partial \boldsymbol{\beta}_t}=\boldsymbol{S} \boldsymbol{\beta}_t-\lambda \boldsymbol{e}-\gamma \boldsymbol{\mu}_t=\mathbf{0}
            \quad \Longrightarrow \quad \boldsymbol{\beta}_t=\boldsymbol{S}^{-1}(\lambda \boldsymbol{e}+\gamma \boldsymbol{\mu}_t)
            \label{eq: optimal portfolio weights}
          \end{align}
          Since $\boldsymbol{\beta}_t^{\top} \boldsymbol{\mu}_t=r_{b, t}$ and $\boldsymbol{\beta}_t^{\top} \boldsymbol{e}=1$, we obtain
          $$
            \begin{aligned}
              e^{\top} \boldsymbol{S}^{-1}(\lambda e+\gamma \boldsymbol{\mu}_t)                  & =1,       \\
              \boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1}(\lambda e+\gamma \boldsymbol{\mu}_t) & =r_{b, t}
            \end{aligned} \quad \Longrightarrow \quad \begin{aligned}
              \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} e \lambda+\boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t \gamma                      & =1,       \\
              \boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1} \boldsymbol{e} \lambda+\boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t \gamma & =r_{b, t}
            \end{aligned}
          $$

          Now, for simplicity, we set $A=\boldsymbol{e}^{\top}
            \boldsymbol{S}^{-1} \boldsymbol{e}, B=\boldsymbol{e}^{\top}
            \boldsymbol{S}^{-1}
            \boldsymbol{\mu}_t=\boldsymbol{\mu}_t^{\top}
            \boldsymbol{S}^{-1} \boldsymbol{e}$, and $C=$
          $\boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1}
            \boldsymbol{\mu}_t$ and obtain
          \begin{align}
            \begin{gathered}
              {\left[\begin{array}{ll}
                    A & B \\
                    B & C
                  \end{array}\right]\left[\begin{array}{l}
                    \lambda \\
                    \gamma
                  \end{array}\right]=\left[\begin{array}{c}
                    1 \\
                    r_{b, t}
                  \end{array}\right]}
              \Longrightarrow
              \left[\begin{array}{l}
                  \lambda^* \\
                  \gamma^*
                \end{array}\right]=\left[\begin{array}{ll}
                  A & B \\
                  B & C
                \end{array}\right]^{-1}\left[\begin{array}{c}
                  1 \\
                  r_{b, t}
                \end{array}\right]=\frac{1}{A C-B^2}\left[\begin{array}{cc}
                  C  & -B \\
                  -B & A
                \end{array}\right]\left[\begin{array}{c}
                  1 \\
                  r_{b, t}
                \end{array}\right]=\frac{1}{\Delta}\left[\begin{array}{c}
                  C-B r_{b, t} \\
                  -B+A r_{b, t}
                \end{array}\right]
            \end{gathered}
            \label{eq: MVO lambda gamma}
          \end{align}
          where $\Delta=A C-B^2$.
          Then, plug in \cref{eq: optimal portfolio weights}, we get
          $$
            \boldsymbol{\beta}_t^*=\lambda^* \boldsymbol{S}^{-1} \boldsymbol{e}+\gamma^* \boldsymbol{S}^{-1} \boldsymbol{\mu}_t
          $$

          This is clearly the minimum as $\frac{\partial^2 L}{\partial \boldsymbol{\beta}_t^2}=\boldsymbol{S}$ is positive definite.

          We call $\boldsymbol{\beta}_t^*$ a mean-variance efficient portfolio/allocation with respect to $r_{b, t}$.

          \textbf{Properties:}
          By construction, the expected return of the optimal portfolio is
          $$
            r_{b, t}=\boldsymbol{\beta}_t^{*^{\top}} \boldsymbol{\mu}_t
          $$

          Some algebra shows that the variance of the optimal portfolio is
          \begin{align*}
            \sigma_{b,t}^{*^2}                          & =\boldsymbol{\beta}_t^{*^{\top}} \boldsymbol{S}\boldsymbol{\beta}_t^*                                                                                                                                                                                                                                                                 \\
                                                        & =\left(\lambda^* \boldsymbol{S}^{-1} \boldsymbol{e}+\gamma^* \boldsymbol{S}^{-1} \boldsymbol{\mu}_t\right)^{\top} \boldsymbol{\Sigma}\left(\lambda^* \boldsymbol{S}^{-1} \boldsymbol{e}+\gamma^* \boldsymbol{S}^{-1} \boldsymbol{\mu}_t\right)                                                                                        \\
                                                        & =\lambda^{*^2} \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{e}+2 \lambda^* \gamma^* \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t+\gamma^{*^2} \boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t                                                                                               \\
                                                        & =\lambda^*\left(\lambda^* \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{e}+\gamma^* \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t\right)+\gamma^*\left(\lambda^* \boldsymbol{e}^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t+\gamma^* \boldsymbol{\mu}_t^{\top} \boldsymbol{S}^{-1} \boldsymbol{\mu}_t\right) \\
                                                        & =\lambda^*\left(\lambda^* A+\gamma^* B\right)+\gamma^*\left(\lambda^* B+\gamma^* C\right)                                                                                                                                                                                                                                             \\
            \cref{eq: MVO lambda gamma} \Longrightarrow & =\lambda^* \cdot 1+\gamma^* \cdot r_{b, t}                                                                                                                                                                                                                                                                                            \\
            \cref{eq: MVO lambda gamma} \Longrightarrow & =\frac{1}{\Delta}\left(C-B r_{b, t}\right)+\frac{1}{\Delta}\left(-B+A r_{b, t}\right) r_{b, t}                                                                                                                                                                                                                                        \\
                                                        & =\frac{A r_{b, t}^2-2 B r_{b, t}+C}{\Delta}
          \end{align*}

          Hence, we have $\sigma_{b,t}^{*^2}$ as a quadratic function of $r_{b, t}$:
          $$
            \sigma_{b,t}^{*^2}=\frac{A r_{b, t}^2-2 B r_{b, t}+C}{\Delta}
          $$

          If we let $r_{b, t}$ vary, the efficient portfolios
          $\boldsymbol{\beta}_t^*$ form a a hyperbola in the
          $\left(\sigma^*\left(r_{b, t}\right), r_{b, t}\right)$ plane and a
          parabola in the $\left(\sigma^{*^2}\left(r_{b, t}\right), r_{b,
              t}\right)$-plane, and we call it the efficient frontier.
          \vspace*{0.5em}

          \hrule

          \textbf{Formulation 2:}

          Since minimizing the given loss function amounts to minimizing the
          MSE which falls under the context of linear regression, we can
          view this index tracking problem as a regression problem. Since
          the covaraince matrix is assumed to be constant over time, we can
          specify a rolling window, say $30$ days, and treat pairs $\{(r_{b,
                t - i}, \mathbf{r}_{t - i}))\}$, $i = 0, 1, \ldots, 29$ as
          samples. Let's denote $\mathbf{y}_t = (r_{b, t}, r_{b, t - 1},
            \ldots, r_{b, t - 29}) \in \mathbb{R}^{30}$ as the regressand, and
          $\mathbf{R}_t = \begin{bmatrix}
              \mathbf{r}_{t}^\top \\
              \vdots              \\
              \mathbf{r}_{t - 29}^\top
            \end{bmatrix}$ as regressors. Then, for time $t$, we can assume
          $$
            \mathbf{y}_t = \mathbf{R}_t \boldsymbol{\beta}_t + \boldsymbol{\epsilon}_t \quad \text{ where }\quad \boldsymbol{\epsilon}_t \sim \mathcal{N}\left(\boldsymbol{0}, \mathbf{\Omega}_t\right)
          $$
          By classical linear regression, we have
          $$
            \boldsymbol{\beta}_t^{*} = \left(\mathbf{R}_t^\top \mathbf{R}_t\right)^{-1}\mathbf{R}_t^\top \mathbf{y}_t
          $$
          By the Gauss-Markov theorem, $\boldsymbol{\beta}_t^{*}$ is the best unbiased linear predictor under the assumptions of classical
          linear regression.
        \end{solution}
  \item In this part, we no longer assume that covariances amongst stocks are
        time invariant. Propose a solution to minimizing the tracking error using the
        Kalman filter.

        \begin{solution}
          Leveraging the powerful Kalman filter, we can update our view on $\boldsymbol{\beta}_t$ from every single new observations on $r_{b, t}$.

          \textbf{State Equation \& Observations Equation:}

          Set $\mathbf{x}_{t} = \boldsymbol{\beta}_t \in \mathbb{R}^{50}$, $\mathbf{F}_{t} = \mathbf{I}_{50}\in \mathbb{R}^{50 \times 50}$,
          $\mathbf{w}_{t}\sim \mathcal{N}\left(\boldsymbol{0}, \mathbf{Q}_{t}\right)$, $\mathbf{Q}_{t} = \boldsymbol{0}$,
          $\mathbf{y}_{t} = r_{b, t}\in \mathbb{R}$,
          $\mathbf{H}_{t} = \mathbf{r}_{t}^\top\in \mathbb{R}^{1\times 50}$,
          $\mathbf{v}_{t} = v \sim \mathcal{N}\left(0, \mathbf{R}_{t}\right)$ and $\mathbf{R}_{t} = 0.3$.

          \textbf{Initial State:}

          Naturally, we set the initial state variable
          $\mathbf{x}_0  \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right)$
          where $\boldsymbol{\mu}_{0} = (1, \ldots, 1)\in \mathbb{R}^{50}$ and $\mathbf{P}_{0} = \mathbf{I}_{50} * 0.1$.

          Therefore, by our construction, we have the linear state space model:
          \begin{align*}
            \begin{aligned}
               & \mathbf{x}_{t}=\mathbf{F}_{t} \mathbf{x}_{t-1}+\mathbf{w}_{t} \\
               & \mathbf{y}_{t}=\mathbf{H}_{t} \mathbf{x}_{t}+\mathbf{v}_{t}
            \end{aligned}\quad \text{ where } \quad
            \mathbf{x}_{0} \sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \mathbf{P}_{0}\right),\quad
            \mathbf{w}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{Q}_{t}\right), \quad
            \mathbf{v}_{t} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{R}_{t}\right)
          \end{align*}
          Additionally, the initial state $\mathbf{x}_{0}$ and the noise terms $\mathbf{w}_{t},
            \mathbf{v}_{t}$ are all assumed to be mutually independent.
        \end{solution}
  \item Download daily market data and create an example that illustrates your
        methodology. How does the tracking portfolio of the Kalman filter perform?

        \begin{solution}
          By comparing errors to $r_{b, t}$, the Kalman filter performs a lot better than the rolling window regression scheme.

          For detailed codes, please check the attached Jupyter notebook.
        \end{solution}
\end{enumerate}
Hint: For (c), compare your Kalman filter to a solution based on (a) where the
covariance matrix is estimated on rolling windows. Can you match the performance
of the Kalman filter with the simpler methodology in (a) by appropriately
choosing the length of the rolling window?
\end{problem}


\section{Topic: Another Interpretation for Independent Component Analysis (ICA)}
\begin{problem}
In class we considered the principally-independent component analysis method
which essentially was the truncated rank- $K$ SVD of a matrix X followed by an
ICA rotation of the left singular components:
$$
  \mathbf{X} \simeq \mathbf{U S V}^\top=\mathbf{U}_I \mathbf{S}_I \mathbf{V}_I
$$
where $\mathbf{U}_I=\mathbf{U} \mathbf{A}_I$ with
$\mathbf{A}_I=\underset{\mathbf{A}, \mathbf{A}^\top
    \mathbf{A}=I_K}{\operatorname{argmax}}\left|k_{\ell}(\mathbf{U A})\right|,
  k_{\ell}(\mathbf{G})$ being any centered cumulant of order $\ell \geq 3$ which
for all practical purposes can be considered a non-linear (activation)

function applied to each of the entries of $\mathbf{G}$. Furthermore the matrix
$\mathbf{V}_I$ was defined as $\mathbf{V}_I^\top:=\mathbf{D}^{-1}
  \mathbf{S}^{-1} \mathbf{A}_I \mathbf{S} \mathbf{V}^\top$ where $\mathbf{D}$
was chosen so that $\mathbf{V}_I$ has unimodular columns.
\begin{enumerate}[label=(\alph*)]
  \item Show that $\mathbf{D}$ is a diagonal matrix
  \item Show that $\mathbf{S}_I=\mathbf{S D}$ is diagonal such that
        $\operatorname{Tr}\left(\mathbf{S}_I^2\right)=\operatorname{Tr}\left(\mathbf{S}^2\right)$.
  \item Show that the method can be derived as the limit, $\lambda^2 \rightarrow
          0$, of the optimization
        $$
          \mathbf{U}_I, \mathbf{S}_I, \mathbf{V}_I=\underset{\mathbf{P}, \mathbf{Q}: \mathbf{P}^\top \mathbf{P}=\operatorname{diag}\left(\mathbf{Q}^\top \mathbf{Q}\right)=\mathbf{I}_k}{\operatorname{argmin}}\left\|\mathbf{X}-\mathbf{P R Q}^\top\right\|_F-\lambda^2\left|k_{\ell}(\mathbf{P})\right| .
        $$
  \item Show that an alternative objective function achieving the same result is
        $$
          \mathbf{U}_I, \mathbf{S}_I, \mathbf{V}_I=\underset{\mathbf{P}, \mathbf{Q}: \mathbf{P}^\top \mathbf{P}=\operatorname{diag}\left(\mathbf{Q}^\top \mathbf{Q}\right)=\mathbf{I}_k}{\operatorname{argmin}}\left\|\mathbf{X}-\mathbf{P R Q}^\top\right\|_F-\lambda^2 J(\mathbf{P}),
        $$
        where $J[\mathbf{x}]:=H\left[\mathbf{x}_{\text {gauss
          }}\right]-H[\mathbf{x}]$ is the negentropy and $J(\mathbf{P})$ is the
        sum of the negentropies of all the columns of $\mathbf{P}$.
\end{enumerate}
\end{problem}














\newpage
\begin{thebibliography}{1}

  \bibitem{Kolm2017}
  Kolm, Petter N. and Ritter, Gordon, On the Bayesian Interpretation of
  Black-Litterman (October 16, 2016). European Journal of Operational Research,
  Volume 258, Issue 2, 16 April 2017, Pages 564-572, Available at SSRN:
  \href{https://ssrn.com/abstract=2853158}{https://ssrn.com/abstract=2853158}.

\end{thebibliography}






\end{document}

